{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tf-gpu",
   "display_name": "TensorFlow-GPU"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "c:\\Users\\thomas.henno\\Desktop\\GitRepos\\course-machine-learning\\week-four\\AS14\ntf.Tensor(b'Hello from TensorFlow 2.1.0', shape=(), dtype=string)\n✔️ Successfully imported all packages\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "random_state = 129763 # In case I need it\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    print(os.getcwd())\n",
    "    import tensorflow as tf\n",
    "    print( tf.constant( 'Hello from TensorFlow ' + tf.__version__ ) )\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from numpy import array\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    print(\"✔️ Successfully imported all packages\")\n",
    "except Exception as ex:\n",
    "    print(\"❌ Could not import one or more libraries:\", ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        0         1         2         3         4         5         6    \\\n0  2521.234  1554.951  1760.839  1874.076  1770.127  2167.416  2250.602   \n1  1944.512  1566.987  1224.113  1500.748  3558.259  2831.670  2078.168   \n2  2045.401  1691.263  4474.665  2021.804  5016.791  1812.510  3736.011   \n3  2165.915  2214.965  2395.277  1909.094  1653.703  2003.161  1731.560   \n4  2745.535  2794.596  2250.799  2202.103  1552.819  2111.839  1948.280   \n\n        7         8         9    ...       673       674       675       676  \\\n0  1567.658  2382.911  1786.066  ...  1329.016  1056.066  1736.332  1544.154   \n1  2216.085  1804.972  2051.166  ...  5592.715  1927.505  2702.186  1364.702   \n2  2855.442  2085.921  1721.896  ...  2012.310  1700.974  1708.750  2220.219   \n3  1907.299  1801.705  1906.117  ...  1215.273  1127.290  2038.196  1871.442   \n4  1637.539  1694.834  1754.551  ...  1832.339  1548.872  1293.723  2272.632   \n\n        677       678       679       680       681       682  \n0  1555.864  1702.201  1854.153  1646.496  1520.763  1368.026  \n1  2801.333  2110.587  2153.170  1852.452  1991.153  2549.164  \n2  1324.229  2571.308  1900.011  2130.460  1793.301  1772.942  \n3  1468.785  1840.928  1682.228  2194.603  2787.753  1712.142  \n4  1986.716  1863.857  1858.556  2018.394  1623.119  1658.063  \n\n[5 rows x 683 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>673</th>\n      <th>674</th>\n      <th>675</th>\n      <th>676</th>\n      <th>677</th>\n      <th>678</th>\n      <th>679</th>\n      <th>680</th>\n      <th>681</th>\n      <th>682</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2521.234</td>\n      <td>1554.951</td>\n      <td>1760.839</td>\n      <td>1874.076</td>\n      <td>1770.127</td>\n      <td>2167.416</td>\n      <td>2250.602</td>\n      <td>1567.658</td>\n      <td>2382.911</td>\n      <td>1786.066</td>\n      <td>...</td>\n      <td>1329.016</td>\n      <td>1056.066</td>\n      <td>1736.332</td>\n      <td>1544.154</td>\n      <td>1555.864</td>\n      <td>1702.201</td>\n      <td>1854.153</td>\n      <td>1646.496</td>\n      <td>1520.763</td>\n      <td>1368.026</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1944.512</td>\n      <td>1566.987</td>\n      <td>1224.113</td>\n      <td>1500.748</td>\n      <td>3558.259</td>\n      <td>2831.670</td>\n      <td>2078.168</td>\n      <td>2216.085</td>\n      <td>1804.972</td>\n      <td>2051.166</td>\n      <td>...</td>\n      <td>5592.715</td>\n      <td>1927.505</td>\n      <td>2702.186</td>\n      <td>1364.702</td>\n      <td>2801.333</td>\n      <td>2110.587</td>\n      <td>2153.170</td>\n      <td>1852.452</td>\n      <td>1991.153</td>\n      <td>2549.164</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2045.401</td>\n      <td>1691.263</td>\n      <td>4474.665</td>\n      <td>2021.804</td>\n      <td>5016.791</td>\n      <td>1812.510</td>\n      <td>3736.011</td>\n      <td>2855.442</td>\n      <td>2085.921</td>\n      <td>1721.896</td>\n      <td>...</td>\n      <td>2012.310</td>\n      <td>1700.974</td>\n      <td>1708.750</td>\n      <td>2220.219</td>\n      <td>1324.229</td>\n      <td>2571.308</td>\n      <td>1900.011</td>\n      <td>2130.460</td>\n      <td>1793.301</td>\n      <td>1772.942</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2165.915</td>\n      <td>2214.965</td>\n      <td>2395.277</td>\n      <td>1909.094</td>\n      <td>1653.703</td>\n      <td>2003.161</td>\n      <td>1731.560</td>\n      <td>1907.299</td>\n      <td>1801.705</td>\n      <td>1906.117</td>\n      <td>...</td>\n      <td>1215.273</td>\n      <td>1127.290</td>\n      <td>2038.196</td>\n      <td>1871.442</td>\n      <td>1468.785</td>\n      <td>1840.928</td>\n      <td>1682.228</td>\n      <td>2194.603</td>\n      <td>2787.753</td>\n      <td>1712.142</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2745.535</td>\n      <td>2794.596</td>\n      <td>2250.799</td>\n      <td>2202.103</td>\n      <td>1552.819</td>\n      <td>2111.839</td>\n      <td>1948.280</td>\n      <td>1637.539</td>\n      <td>1694.834</td>\n      <td>1754.551</td>\n      <td>...</td>\n      <td>1832.339</td>\n      <td>1548.872</td>\n      <td>1293.723</td>\n      <td>2272.632</td>\n      <td>1986.716</td>\n      <td>1863.857</td>\n      <td>1858.556</td>\n      <td>2018.394</td>\n      <td>1623.119</td>\n      <td>1658.063</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 683 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_excel(\"Dataset.xlsx\", skiprows=2, header=None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.aggregate(\"mean\", axis=\"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2178.962833333333, 2140.8261666666663, 2029.3010833333335, 1878.8448611111114, 2105.957083333333, 1998.5725555555557, 2226.943861111111, 1938.2611944444445, 1807.1166666666666, 1781.9076111111108, 1794.381, 1867.543138888889, 1881.5988888888892, 1788.7648333333334, 1769.7328888888887, 1778.89275, 1750.3532777777777, 1744.098472222222, 1605.8494722222222, 1602.4786388888888, 1610.041694444445, 1582.7103333333334, 1754.1105, 1821.1048333333335, 2056.823611111111, 1912.785083333334, 1761.8987499999998, 1809.4357499999996, 2006.3628055555555, 1858.2195555555554, 1952.6886666666662, 2025.8715000000002, 1913.2763055555556, 1765.9903888888887, 1763.3245555555554, 1827.8902777777778, 1909.5682499999998, 1799.694388888889, 1741.173055555556, 1827.0989444444447, 1816.642388888889, 1829.5460277777775, 1801.1698333333334, 1763.7866111111111, 1595.841666666667, 1623.936027777778, 1614.531388888889, 1865.5651388888884, 1999.4232777777777, 2141.1095, 2113.6754999999994, 1858.3847777777778, 1857.6436944444447, 2009.211611111111, 2043.3201388888895, 2038.3976666666665, 1936.254916666667, 1880.3626666666667, 1807.5786944444446, 1798.6267777777775, 1771.5132777777774, 1697.9339722222223, 1871.1449166666669, 1792.2293055555556, 1848.7482222222218, 1908.7749166666663, 1862.003972222223, 1802.2375833333335, 1688.3722777777775, 1758.1148611111114, 1853.4748333333332, 2093.3103333333333, 2249.4082499999995, 2042.0869444444443, 2083.156499999999, 2019.7374999999997, 2123.7890555555555, 2177.572194444445, 2259.0851111111115, 2216.529138888889, 2098.8821666666668, 2003.8931666666665, 1848.5690833333335, 2033.0949999999996, 1988.6234444444444, 1930.443416666666, 1795.4996388888892, 1874.7705833333337, 1877.5836388888888, 1857.9982222222222, 1908.7273611111111, 1737.0369166666665, 1826.3157777777778, 1786.9690555555558, 1778.4312777777782, 2011.2130555555555, 2413.391194444444, 2385.0911388888885, 1992.0776111111109, 1891.045666666667, 2089.2423333333336, 1929.9350833333333, 2038.7761666666665, 2128.6828888888895, 2057.370138888889, 2086.607055555556, 2022.9001666666668, 1959.406083333333, 1896.2050555555559, 1933.308083333333, 1847.8386388888891, 1777.7825833333334, 1774.905083333333, 1831.8730833333336, 1726.9209722222226, 1735.4833611111108, 1763.0475555555556, 1703.662916666667, 1757.2707777777778, 1941.6315555555557, 2240.3052500000003, 2104.866611111111, 1795.6743333333334, 1657.9416388888887, 1579.9677222222222, 1765.9276111111112, 1861.845972222222, 1866.2226666666672, 1890.97875, 1870.6765833333334, 1839.5219166666664, 1821.2589722222224, 1775.3780555555556, 1876.3753611111113, 1934.2833055555554, 1829.3748888888888, 1880.4591944444446, 1870.3043055555556, 1843.9672777777782, 1675.3174999999997, 1691.2366111111112, 1670.719111111111, 1783.6936666666668, 1985.835555555556, 2021.7055833333336, 1866.8839444444436, 1934.4641111111112, 1932.7959722222222, 1904.0947499999997, 2000.7141666666669, 2002.978416666667, 1816.7739166666665, 1949.0788333333335, 1772.0579166666664, 1769.1450277777776, 1785.0483611111106, 1797.0914999999995, 1789.4283888888895, 1773.8118611111108, 1847.0957777777778, 1845.5650277777777, 1843.3693888888884, 1774.899222222222, 1661.9573611111118, 1652.1412222222223, 1559.2772777777782, 1705.7639722222218, 2082.3124166666666, 2302.6712777777775, 2231.635916666667, 1815.8536666666666, 1942.9438611111118, 2011.1102777777778, 2010.6860555555556, 1942.8210833333333, 1842.3017500000003, 1763.79525, 1854.0023333333334, 1865.162916666667, 1938.147611111111, 1942.5567222222223, 1766.1631944444448, 1771.2105277777775, 1767.2593333333332, 1758.6058055555557, 1744.0128055555558, 1687.946027777778, 1674.836944444444, 1701.6101666666664, 1537.6534166666665, 1638.6082222222221, 1906.593972222222, 2039.2111666666667, 2019.7236388888891, 1919.1986666666671, 1879.2242500000002, 1919.3648055555557, 1908.0576388888892, 2158.5803055555552, 2027.3320277777775, 1825.0845, 1893.7248055555558, 1820.6770555555554, 1873.5607777777777, 1868.3872499999993, 1667.231888888889, 1763.3501111111113, 1858.4104166666662, 1694.0316388888884, 1782.5168611111117, 1869.0332500000004, 1729.0525277777774, 1668.8290833333333, 1675.9292500000001, 1745.0586666666666, 1952.8002777777776, 1950.2158333333334, 2162.218666666666, 2048.903777777777, 2006.1834166666672, 2258.138277777778, 2228.745916666667, 2045.0932500000001, 2083.47625, 2006.3535555555547, 1913.5060833333332, 1884.9785000000002, 1915.4474722222221, 1849.1367777777773, 1787.0480833333334, 1808.9574444444443, 1777.931916666667, 1793.7198055555555, 1751.6232499999996, 1785.2536944444446, 1696.9295, 1667.3255833333328, 1753.9730000000002, 1760.817027777778, 1992.978111111111, 2103.2310555555555, 2032.815277777778, 2090.137166666667, 2069.5292777777777, 1960.3520833333334, 1926.7587222222226, 2109.390888888889, 1917.0811666666668, 1971.747361111111, 2016.3933611111113, 1883.9820277777783, 1905.2825555555557, 2010.1291111111107, 1841.9461666666664, 1855.4331388888886, 1884.6995277777771, 1782.3601666666668, 1786.1333055555558, 2002.3865, 1770.5004722222225, 1684.3338888888889, 1668.855416666667, 1738.740027777778, 1900.3599166666665, 2128.058611111111, 2158.931944444444, 1929.7252222222223, 1821.0850833333336, 1798.6015, 1911.0637222222224, 2099.243777777778, 1872.5230000000001, 1884.1780277777777, 1898.1970555555558, 1937.6123055555554, 2029.6248888888886, 1880.1286666666665, 1873.9370277777775, 1827.777583333333, 1925.5336111111105, 1858.4550277777776, 1887.2939444444446, 1832.8076944444447, 1883.9312500000003, 1678.9855, 1661.0727222222224, 1747.9448888888887, 1969.6772222222219, 1984.252611111111, 2097.250194444445, 1972.1521944444441, 1986.9411111111112, 1988.0627777777781, 2321.155888888888, 2261.3455833333323, 1974.6004166666664, 1896.1972222222223, 1915.7568888888886, 1895.336222222223, 1813.9030833333331, 1863.3296111111113, 1864.5354444444447, 1808.6411944444442, 1847.2544166666667, 1799.6565555555555, 1897.489305555555, 1782.1907222222223, 1667.7458055555558, 1677.7325555555556, 1627.3766388888887, 1588.4370277777784, 1813.4160277777778, 2113.589722222222, 2060.907777777778, 1973.3475277777777, 1802.0770277777776, 1960.5359999999996, 1809.956527777778, 1807.6986111111105, 1866.582333333333, 1773.9505000000004, 1708.8333055555556, 1733.2134444444446, 1706.2701944444445, 1792.5374722222225, 1641.744472222222, 1855.5683888888893, 1971.8002777777776, 1834.9032500000003, 1736.5807777777782, 1711.6593055555554, 1717.7161388888887, 1720.6729444444447, 1587.9881111111115, 1701.4238333333333, 1915.0133055555557, 2271.103472222222, 2462.0076666666664, 2224.4925, 1887.7314999999996, 2156.3996111111105, 2231.419361111111, 2143.5006666666663, 1996.6084722222217, 1778.2296944444445, 1775.5113333333331, 1732.2687777777778, 1837.0248055555555, 1885.7648055555555, 1909.403472222222, 1819.2706111111115, 1868.391, 1831.6796666666664, 1770.2697777777778, 1807.760305555555, 1742.130361111111, 1721.3872777777776, 1672.672, 1806.666083333333, 2083.149194444444, 2308.066444444445, 2190.810333333333, 1898.9196111111112, 2136.6126388888893, 2080.8612777777776, 2310.7096666666666, 2243.7109722222226, 2133.996611111111, 2040.1515833333328, 1849.1733055555555, 1975.9448055555554, 1915.7264444444445, 1961.7038888888883, 1950.3721666666668, 1857.0649166666672, 1937.502916666667, 1897.737638888889, 1903.3350833333332, 1913.854444444444, 1739.019527777778, 1665.6244444444446, 1690.8301111111114, 1705.5684444444446, 1970.1371388888886, 2101.514666666667, 2283.7687222222226, 1979.9494166666664, 1949.3246944444445, 1807.1727777777783, 2123.820388888889, 2115.560166666666, 1985.4685555555557, 1983.0227499999996, 1910.602972222222, 1924.2256388888886, 1921.3405277777779, 1797.6620833333334, 1754.8618888888896, 1716.5608611111104, 1730.3346111111111, 1766.5149444444442, 1790.207, 1885.619, 1686.177277777778, 1665.9953888888886, 1712.7573333333335, 1758.858777777778, 1991.0392777777774, 2162.9873611111116, 2146.5076944444436, 2076.347694444444, 1893.4616944444444, 1983.1145555555559, 2072.674138888888, 2342.7510000000007, 2173.681777777778, 1819.484361111111, 1911.4896388888887, 1784.3795, 1823.7957222222221, 1845.2818888888885, 1920.5754999999995, 1942.310527777778, 1873.5365, 1902.1559722222223, 1669.9531666666667, 1756.7010555555553, 1676.2982500000003, 1746.5463611111109, 1710.1660277777778, 1775.528722222222, 2003.4586944444445, 2347.1877222222224, 2212.7499166666666, 2046.77, 1831.686722222223, 1508.75425, 1907.5119166666664, 1882.2128055555559, 2037.295111111111, 1939.2445555555557, 1961.551083333333, 1941.2520000000004, 1933.6922499999996, 1949.609861111111, 1913.6617222222226, 1954.1163055555562, 1754.1171944444452, 1842.7234166666667, 1886.0440277777775, 1837.1317499999996, 1680.5942222222225, 1751.0449444444444, 1637.2869166666665, 1656.0619166666668, 1909.4082777777778, 2074.477944444444, 2099.5509166666666, 1863.822805555556, 1750.6704444444445, 1922.9068888888885, 1864.5496666666668, 1901.249833333333, 2028.5153055555554, 2060.513055555556, 1949.9028888888893, 1907.3447777777776, 1897.1928333333335, 1893.5721388888892, 1948.0888055555552, 1970.9329166666664, 1911.9193888888888, 1836.2361111111106, 1795.4754444444445, 1880.255, 1754.0612777777776, 1713.123944444444, 1656.7919166666663, 1780.4781111111108, 2016.5205833333332, 2122.677, 2421.7078611111106, 2144.3925, 2104.4176666666663, 1883.7819999999997, 1976.9186388888888, 2174.7051388888885, 2083.5482499999994, 1947.4013888888887, 1901.5889722222228, 1901.0812222222226, 1861.0273888888887, 1828.1251111111112, 1769.6969444444444, 1666.8382500000002, 1798.7750833333332, 1767.8048333333338, 1784.257833333333, 1713.9526666666666, 1715.1912499999999, 1673.0140555555554, 1631.8810833333328, 1737.246583333333, 2094.126916666666, 2093.7761666666665, 1984.5935277777778, 1834.9000277777782, 1907.2042500000005, 1998.961666666667, 2035.3187500000001, 2039.1583333333338, 1893.0142499999997, 1908.5537222222224, 2029.91975, 1916.070361111111, 1846.8975555555558, 1814.6399722222222, 1875.4483611111111, 1800.608666666666, 1777.361277777778, 1747.2423055555557, 1722.8412222222223, 1790.2811388888886, 1700.50025, 1697.424, 1568.049388888889, 1643.0216388888887, 1940.4026111111114, 2237.174222222222, 2270.463861111112, 2274.101083333333, 2095.5268611111114, 2068.053333333333, 2228.430027777778, 2090.3205, 2098.825555555556, 2041.5785555555562, 1859.1721388888889, 1914.2313055555553, 1909.4480833333328, 1942.2615555555558, 1896.132361111111, 1836.9292500000001, 1845.9246111111113, 1809.8781388888888, 1736.4569722222222, 1736.6820555555555, 1741.0494166666667, 1662.1548333333337, 1661.1480833333333, 1731.9198055555557, 1983.8030277777782, 2120.567138888889, 1831.3429166666667, 1961.5916944444443, 1813.7932499999997, 1809.7198888888893, 1974.4131111111103, 2005.2418333333335, 1948.449083333333, 1921.2616666666675, 1858.6453333333334, 1852.1131944444448, 1918.6699444444441, 1999.3080555555555, 1750.8667500000001, 1845.6397222222222, 1807.4533055555557, 1752.5980833333333, 1782.3708611111113, 1839.5782499999998, 1709.5820555555556, 1853.0048055555558, 1798.235388888889, 1871.9463333333333, 2038.1791388888882, 2287.6301666666677, 2304.088277777778, 2231.493, 2056.596027777778, 2066.9811111111107, 2043.7948611111112, 2063.6095000000005, 2068.0416944444446, 2123.5398333333337, 1974.4216388888894, 1899.7449722222223, 1883.5685, 2017.1003611111114, 2012.5995833333336, 1940.0294722222222, 1875.5226388888887, 1895.314083333333, 1818.1725555555558, 1769.7798888888883, 1691.3580000000002, 1742.6850833333333, 1702.1673055555561, 1735.9231666666667, 1983.5559999999994, 2289.2303611111115, 2257.6233055555554, 1929.0263888888887, 2016.0975000000003, 1965.498472222222, 1956.6487499999994, 2066.350805555556, 2030.1858888888887, 2052.3951944444443, 1942.418138888889, 1970.552638888889, 1981.2108055555561, 1899.1468333333335, 1859.9037500000002, 1946.0254444444445, 1864.6312499999997, 1777.711583333334, 1762.4231666666672, 1754.7711666666667, 1717.1704166666668, 1688.9349999999995, 1731.4519166666664, 1708.7353611111112, 1946.6093333333342, 2174.154638888889, 2104.147694444445, 2053.871722222222, 1942.4058888888894, 1761.4298055555557, 1962.4699999999996, 1982.2071666666666, 2072.8683055555557, 1922.5884722222227, 1965.181972222222, 1732.7252777777778, 1789.8473611111115, 1797.616222222222, 1733.2769722222222, 1758.1311944444446, 1734.7841944444447, 1679.4603611111115, 1714.6155833333337, 1671.0885000000005, 1655.5635000000007, 1711.1044166666668, 1721.7251111111113, 1776.5138333333327, 2118.953, 2171.4619166666666, 2065.427555555556, 1924.605722222222, 1951.9800833333345, 2154.058083333333, 1929.3857777777775, 2045.247722222222, 1989.0830277777782, 1953.7858055555557, 1928.481361111111, 1942.9790833333334, 1908.5405555555558, 1818.4195, 1842.8397499999996, 1772.5461111111113, 1730.5121388888888, 1888.7372777777778, 1792.3740277777774, 1776.1009999999999, 1763.6137777777776, 1720.57475, 1620.0719166666668, 1724.0548055555555, 1979.3395555555555, 2178.7780000000002, 2169.7128611111116, 2022.0994999999998, 2075.3754166666663, 2356.756083333333, 2020.1112499999997, 1979.140722222222, 1948.1131111111108, 2016.6179166666666, 1908.7975277777778, 1952.303444444444]\n"
    }
   ],
   "source": [
    "sequence = [x for x in df.values.tolist()]\n",
    "print(sequence)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'> 504\n<class 'numpy.ndarray'> 131\n<class 'numpy.ndarray'> 504\n<class 'numpy.ndarray'> 131\n"
    }
   ],
   "source": [
    "n_steps = 48\n",
    "\n",
    "# Split into samples\n",
    "X, y = split_sequence(sequence, n_steps)\n",
    "\n",
    "X_train = X[0:504]\n",
    "y_train = y[0:504]\n",
    "\n",
    "X_test = X[504:]\n",
    "y_test = y[504:]\n",
    "\n",
    "print(type(X_train), len(X_train))\n",
    "print(type(X_test), len(X_test))\n",
    "print(type(y_train), len(y_train))\n",
    "print(type(y_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(504, 48, 1)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "n_features = 1\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/200\n504/504 [==============================] - 1s 1ms/step - loss: 5511714.8175\nEpoch 2/200\n504/504 [==============================] - 0s 752us/step - loss: 3975890.5238\nEpoch 3/200\n504/504 [==============================] - 0s 726us/step - loss: 2562058.5972\nEpoch 4/200\n504/504 [==============================] - 0s 730us/step - loss: 4244261.8016\nEpoch 5/200\n504/504 [==============================] - 0s 730us/step - loss: 2680563.7063\nEpoch 6/200\n504/504 [==============================] - 0s 722us/step - loss: 851047.7540\nEpoch 7/200\n504/504 [==============================] - 0s 744us/step - loss: 767227.4444\nEpoch 8/200\n504/504 [==============================] - 0s 760us/step - loss: 721021.3770\nEpoch 9/200\n504/504 [==============================] - 0s 746us/step - loss: 602225.7604\nEpoch 10/200\n504/504 [==============================] - 0s 740us/step - loss: 544183.3938\nEpoch 11/200\n504/504 [==============================] - 0s 728us/step - loss: 592472.4945\nEpoch 12/200\n504/504 [==============================] - 0s 734us/step - loss: 428460.2222\nEpoch 13/200\n504/504 [==============================] - 0s 730us/step - loss: 500512.3338\nEpoch 14/200\n504/504 [==============================] - 0s 732us/step - loss: 612866.3611\nEpoch 15/200\n504/504 [==============================] - 0s 734us/step - loss: 633496.3413\nEpoch 16/200\n504/504 [==============================] - 0s 734us/step - loss: 544362.6949\nEpoch 17/200\n504/504 [==============================] - 0s 732us/step - loss: 481979.2277\nEpoch 18/200\n504/504 [==============================] - 0s 728us/step - loss: 666207.0506\nEpoch 19/200\n504/504 [==============================] - 0s 750us/step - loss: 684633.8323\nEpoch 20/200\n504/504 [==============================] - 0s 722us/step - loss: 642866.2966\nEpoch 21/200\n504/504 [==============================] - 0s 736us/step - loss: 497306.1746\nEpoch 22/200\n504/504 [==============================] - 0s 736us/step - loss: 454461.5506\nEpoch 23/200\n504/504 [==============================] - 0s 740us/step - loss: 400731.3438\nEpoch 24/200\n504/504 [==============================] - 0s 738us/step - loss: 343226.8631\nEpoch 25/200\n504/504 [==============================] - 0s 736us/step - loss: 317342.3041\nEpoch 26/200\n504/504 [==============================] - 0s 744us/step - loss: 297962.7304\nEpoch 27/200\n504/504 [==============================] - 0s 734us/step - loss: 281098.0913\nEpoch 28/200\n504/504 [==============================] - 0s 714us/step - loss: 253790.0856\nEpoch 29/200\n504/504 [==============================] - 0s 738us/step - loss: 257164.3080\nEpoch 30/200\n504/504 [==============================] - 0s 732us/step - loss: 257832.8807\nEpoch 31/200\n504/504 [==============================] - 0s 742us/step - loss: 217612.5618\nEpoch 32/200\n504/504 [==============================] - 0s 740us/step - loss: 231644.0570\nEpoch 33/200\n504/504 [==============================] - 0s 740us/step - loss: 209321.4722\nEpoch 34/200\n504/504 [==============================] - 0s 744us/step - loss: 188579.6203\nEpoch 35/200\n504/504 [==============================] - 0s 736us/step - loss: 189133.8375\nEpoch 36/200\n504/504 [==============================] - 0s 734us/step - loss: 184755.0890\nEpoch 37/200\n504/504 [==============================] - 0s 740us/step - loss: 193404.1376\nEpoch 38/200\n504/504 [==============================] - 0s 724us/step - loss: 203553.4563\nEpoch 39/200\n504/504 [==============================] - 0s 722us/step - loss: 204271.4735\nEpoch 40/200\n504/504 [==============================] - 0s 754us/step - loss: 230926.3388\nEpoch 41/200\n504/504 [==============================] - 0s 722us/step - loss: 1310446.5060\nEpoch 42/200\n504/504 [==============================] - 0s 732us/step - loss: 1708035.5992\nEpoch 43/200\n504/504 [==============================] - 0s 714us/step - loss: 1659742.5263\nEpoch 44/200\n504/504 [==============================] - 0s 722us/step - loss: 418322.7688\nEpoch 45/200\n504/504 [==============================] - 0s 728us/step - loss: 154635.2026\nEpoch 46/200\n504/504 [==============================] - 0s 728us/step - loss: 112083.2295\nEpoch 47/200\n504/504 [==============================] - 0s 740us/step - loss: 102298.8144\nEpoch 48/200\n504/504 [==============================] - 0s 730us/step - loss: 133930.9670\nEpoch 49/200\n504/504 [==============================] - 0s 718us/step - loss: 118326.9436\nEpoch 50/200\n504/504 [==============================] - 0s 728us/step - loss: 116075.6011\nEpoch 51/200\n504/504 [==============================] - 0s 730us/step - loss: 124224.5115\nEpoch 52/200\n504/504 [==============================] - 0s 720us/step - loss: 110423.4975\nEpoch 53/200\n504/504 [==============================] - 0s 738us/step - loss: 106166.5516\nEpoch 54/200\n504/504 [==============================] - 0s 718us/step - loss: 113857.1818\nEpoch 55/200\n504/504 [==============================] - 0s 734us/step - loss: 114543.5589\nEpoch 56/200\n504/504 [==============================] - 0s 744us/step - loss: 100078.1255\nEpoch 57/200\n504/504 [==============================] - 0s 738us/step - loss: 92865.6762\nEpoch 58/200\n504/504 [==============================] - 0s 722us/step - loss: 88989.3049\nEpoch 59/200\n504/504 [==============================] - 0s 724us/step - loss: 93584.0544\nEpoch 60/200\n504/504 [==============================] - 0s 726us/step - loss: 87453.0145\nEpoch 61/200\n504/504 [==============================] - 0s 728us/step - loss: 78641.1814\nEpoch 62/200\n504/504 [==============================] - 0s 744us/step - loss: 88819.4261\nEpoch 63/200\n504/504 [==============================] - 0s 730us/step - loss: 82874.2588\nEpoch 64/200\n504/504 [==============================] - 0s 726us/step - loss: 76752.4233\nEpoch 65/200\n504/504 [==============================] - 0s 728us/step - loss: 72787.7063\nEpoch 66/200\n504/504 [==============================] - 0s 724us/step - loss: 73718.3306\nEpoch 67/200\n504/504 [==============================] - 0s 726us/step - loss: 126858.6163\nEpoch 68/200\n504/504 [==============================] - 0s 701us/step - loss: 109462.6371\nEpoch 69/200\n504/504 [==============================] - 0s 734us/step - loss: 110652.0383\nEpoch 70/200\n504/504 [==============================] - 0s 746us/step - loss: 102849.3327\nEpoch 71/200\n504/504 [==============================] - 0s 740us/step - loss: 105891.7435\nEpoch 72/200\n504/504 [==============================] - 0s 730us/step - loss: 48977.4185\nEpoch 73/200\n504/504 [==============================] - 0s 736us/step - loss: 46487.4437\nEpoch 74/200\n504/504 [==============================] - 0s 762us/step - loss: 50972.6066\nEpoch 75/200\n504/504 [==============================] - 0s 722us/step - loss: 47759.1564\nEpoch 76/200\n504/504 [==============================] - 0s 740us/step - loss: 45382.9746\nEpoch 77/200\n504/504 [==============================] - 0s 740us/step - loss: 51100.5465\nEpoch 78/200\n504/504 [==============================] - 0s 732us/step - loss: 83431.2047\nEpoch 79/200\n504/504 [==============================] - 0s 748us/step - loss: 88444.1808\nEpoch 80/200\n504/504 [==============================] - 0s 752us/step - loss: 100938.5045\nEpoch 81/200\n504/504 [==============================] - 0s 752us/step - loss: 99314.8967\nEpoch 82/200\n504/504 [==============================] - 0s 742us/step - loss: 87500.0604\nEpoch 83/200\n504/504 [==============================] - 0s 740us/step - loss: 83370.6335\nEpoch 84/200\n504/504 [==============================] - 0s 732us/step - loss: 84645.4845\nEpoch 85/200\n504/504 [==============================] - 0s 724us/step - loss: 72574.1693\nEpoch 86/200\n504/504 [==============================] - 0s 750us/step - loss: 80946.7558\nEpoch 87/200\n504/504 [==============================] - 0s 764us/step - loss: 86767.6038\nEpoch 88/200\n504/504 [==============================] - 0s 718us/step - loss: 81339.8218\nEpoch 89/200\n504/504 [==============================] - 0s 746us/step - loss: 79854.4180\nEpoch 90/200\n504/504 [==============================] - 0s 746us/step - loss: 74595.5095\nEpoch 91/200\n504/504 [==============================] - 0s 746us/step - loss: 75884.9681\nEpoch 92/200\n504/504 [==============================] - 0s 734us/step - loss: 69306.9932\nEpoch 93/200\n504/504 [==============================] - 0s 734us/step - loss: 61017.8591\nEpoch 94/200\n504/504 [==============================] - 0s 730us/step - loss: 57137.9549\nEpoch 95/200\n504/504 [==============================] - 0s 746us/step - loss: 51967.0468\nEpoch 96/200\n504/504 [==============================] - 0s 740us/step - loss: 53128.2798\nEpoch 97/200\n504/504 [==============================] - 0s 762us/step - loss: 51718.2515\nEpoch 98/200\n504/504 [==============================] - 0s 748us/step - loss: 47427.1720\nEpoch 99/200\n504/504 [==============================] - 0s 720us/step - loss: 48453.7750\nEpoch 100/200\n504/504 [==============================] - 0s 720us/step - loss: 40693.0495\nEpoch 101/200\n504/504 [==============================] - 0s 732us/step - loss: 36089.5014\nEpoch 102/200\n504/504 [==============================] - 0s 734us/step - loss: 36756.7389\nEpoch 103/200\n504/504 [==============================] - 0s 746us/step - loss: 36000.2152\nEpoch 104/200\n504/504 [==============================] - 0s 756us/step - loss: 36392.9193\nEpoch 105/200\n504/504 [==============================] - 0s 744us/step - loss: 35059.6880\nEpoch 106/200\n504/504 [==============================] - 0s 752us/step - loss: 34300.4713\nEpoch 107/200\n504/504 [==============================] - 0s 736us/step - loss: 32876.3283\nEpoch 108/200\n504/504 [==============================] - 0s 732us/step - loss: 32950.0133\nEpoch 109/200\n504/504 [==============================] - 0s 724us/step - loss: 33825.6364\nEpoch 110/200\n504/504 [==============================] - 0s 714us/step - loss: 33529.1809\nEpoch 111/200\n504/504 [==============================] - 0s 728us/step - loss: 32838.2615\nEpoch 112/200\n504/504 [==============================] - 0s 730us/step - loss: 33005.4749\nEpoch 113/200\n504/504 [==============================] - 0s 728us/step - loss: 32831.7912\nEpoch 114/200\n504/504 [==============================] - 0s 742us/step - loss: 32680.5665\nEpoch 115/200\n504/504 [==============================] - 0s 752us/step - loss: 32098.8562\nEpoch 116/200\n504/504 [==============================] - 0s 758us/step - loss: 32110.6497\nEpoch 117/200\n504/504 [==============================] - 0s 754us/step - loss: 32434.0111\nEpoch 118/200\n504/504 [==============================] - 0s 734us/step - loss: 32300.5942\nEpoch 119/200\n504/504 [==============================] - 0s 732us/step - loss: 32487.5025\nEpoch 120/200\n504/504 [==============================] - 0s 724us/step - loss: 33281.8480\nEpoch 121/200\n504/504 [==============================] - 0s 736us/step - loss: 33566.3488\nEpoch 122/200\n504/504 [==============================] - 0s 724us/step - loss: 32881.0217\nEpoch 123/200\n504/504 [==============================] - 0s 740us/step - loss: 32777.4692\nEpoch 124/200\n504/504 [==============================] - 0s 742us/step - loss: 33060.0374\nEpoch 125/200\n504/504 [==============================] - 0s 728us/step - loss: 32546.9583\nEpoch 126/200\n504/504 [==============================] - 0s 734us/step - loss: 33179.7992\nEpoch 127/200\n504/504 [==============================] - 0s 734us/step - loss: 32113.8506\nEpoch 128/200\n504/504 [==============================] - 0s 726us/step - loss: 32346.0687\nEpoch 129/200\n504/504 [==============================] - 0s 738us/step - loss: 33773.7783\nEpoch 130/200\n504/504 [==============================] - 0s 738us/step - loss: 33909.8463\nEpoch 131/200\n504/504 [==============================] - 0s 730us/step - loss: 34197.8806\nEpoch 132/200\n504/504 [==============================] - 0s 736us/step - loss: 31391.9435\nEpoch 133/200\n504/504 [==============================] - 0s 740us/step - loss: 35325.0396\nEpoch 134/200\n504/504 [==============================] - 0s 724us/step - loss: 35876.6913\nEpoch 135/200\n504/504 [==============================] - 0s 748us/step - loss: 34934.1823\nEpoch 136/200\n504/504 [==============================] - 0s 724us/step - loss: 35254.1414\nEpoch 137/200\n504/504 [==============================] - 0s 740us/step - loss: 36570.2037\nEpoch 138/200\n504/504 [==============================] - 0s 736us/step - loss: 36644.7283\nEpoch 139/200\n504/504 [==============================] - 0s 712us/step - loss: 37459.3232\nEpoch 140/200\n504/504 [==============================] - 0s 728us/step - loss: 36762.9019\nEpoch 141/200\n504/504 [==============================] - 0s 722us/step - loss: 35612.2553\nEpoch 142/200\n504/504 [==============================] - 0s 754us/step - loss: 36277.5549\nEpoch 143/200\n504/504 [==============================] - 0s 732us/step - loss: 37240.9746\nEpoch 144/200\n504/504 [==============================] - 0s 746us/step - loss: 36216.1067\nEpoch 145/200\n504/504 [==============================] - 0s 716us/step - loss: 34773.7162\nEpoch 146/200\n504/504 [==============================] - 0s 740us/step - loss: 34836.8968\nEpoch 147/200\n504/504 [==============================] - 0s 730us/step - loss: 34771.8158\nEpoch 148/200\n504/504 [==============================] - 0s 738us/step - loss: 35762.6135\nEpoch 149/200\n504/504 [==============================] - 0s 732us/step - loss: 35870.8720\nEpoch 150/200\n504/504 [==============================] - 0s 720us/step - loss: 33121.5634\nEpoch 151/200\n504/504 [==============================] - 0s 712us/step - loss: 33704.2207\nEpoch 152/200\n504/504 [==============================] - 0s 734us/step - loss: 31610.6185\nEpoch 153/200\n504/504 [==============================] - 0s 718us/step - loss: 30888.8861\nEpoch 154/200\n504/504 [==============================] - 0s 768us/step - loss: 33716.9735\nEpoch 155/200\n504/504 [==============================] - 0s 758us/step - loss: 36254.7605\nEpoch 156/200\n504/504 [==============================] - 0s 738us/step - loss: 33222.6540\nEpoch 157/200\n504/504 [==============================] - 0s 730us/step - loss: 31474.2265\nEpoch 158/200\n504/504 [==============================] - 0s 740us/step - loss: 34489.6213\nEpoch 159/200\n504/504 [==============================] - 0s 730us/step - loss: 39413.2308\nEpoch 160/200\n504/504 [==============================] - 0s 758us/step - loss: 40014.3031\nEpoch 161/200\n504/504 [==============================] - 0s 730us/step - loss: 32637.0807\nEpoch 162/200\n504/504 [==============================] - 0s 720us/step - loss: 34727.7208\nEpoch 163/200\n504/504 [==============================] - 0s 734us/step - loss: 36644.3875\nEpoch 164/200\n504/504 [==============================] - 0s 724us/step - loss: 36561.5570\nEpoch 165/200\n504/504 [==============================] - 0s 730us/step - loss: 35714.9480\nEpoch 166/200\n504/504 [==============================] - 0s 718us/step - loss: 39758.5549\nEpoch 167/200\n504/504 [==============================] - 0s 730us/step - loss: 40420.1683\nEpoch 168/200\n504/504 [==============================] - 0s 736us/step - loss: 40275.9023\nEpoch 169/200\n504/504 [==============================] - 0s 734us/step - loss: 39062.8147\nEpoch 170/200\n504/504 [==============================] - 0s 742us/step - loss: 37231.7899\nEpoch 171/200\n504/504 [==============================] - 0s 752us/step - loss: 38367.8492\nEpoch 172/200\n504/504 [==============================] - 0s 772us/step - loss: 37809.9295\nEpoch 173/200\n504/504 [==============================] - 0s 758us/step - loss: 35985.6392\nEpoch 174/200\n504/504 [==============================] - 0s 768us/step - loss: 35807.8161\nEpoch 175/200\n504/504 [==============================] - 0s 815us/step - loss: 34458.7280\nEpoch 176/200\n504/504 [==============================] - 0s 756us/step - loss: 32608.9253\nEpoch 177/200\n504/504 [==============================] - 0s 740us/step - loss: 32000.9546\nEpoch 178/200\n504/504 [==============================] - 0s 728us/step - loss: 30523.2890\nEpoch 179/200\n504/504 [==============================] - 0s 744us/step - loss: 30111.2595\nEpoch 180/200\n504/504 [==============================] - 0s 722us/step - loss: 30320.6804\nEpoch 181/200\n504/504 [==============================] - 0s 734us/step - loss: 29348.9178\nEpoch 182/200\n504/504 [==============================] - 0s 746us/step - loss: 29032.0162\nEpoch 183/200\n504/504 [==============================] - 0s 734us/step - loss: 28377.0184\nEpoch 184/200\n504/504 [==============================] - 0s 750us/step - loss: 27505.8454\nEpoch 185/200\n504/504 [==============================] - 0s 720us/step - loss: 27122.2457\nEpoch 186/200\n504/504 [==============================] - 0s 742us/step - loss: 27117.8710\nEpoch 187/200\n504/504 [==============================] - 0s 732us/step - loss: 27313.9066\nEpoch 188/200\n504/504 [==============================] - 0s 730us/step - loss: 25740.4610\nEpoch 189/200\n504/504 [==============================] - 0s 732us/step - loss: 25528.0318\nEpoch 190/200\n504/504 [==============================] - 0s 809us/step - loss: 25449.9974\nEpoch 191/200\n504/504 [==============================] - 0s 746us/step - loss: 25848.7094\nEpoch 192/200\n504/504 [==============================] - 0s 752us/step - loss: 26060.7200\nEpoch 193/200\n504/504 [==============================] - 0s 760us/step - loss: 25672.8155\nEpoch 194/200\n504/504 [==============================] - 0s 738us/step - loss: 25762.7910\nEpoch 195/200\n504/504 [==============================] - 0s 746us/step - loss: 26950.9496\nEpoch 196/200\n504/504 [==============================] - 0s 718us/step - loss: 27313.1889\nEpoch 197/200\n504/504 [==============================] - 0s 740us/step - loss: 26006.1428\nEpoch 198/200\n504/504 [==============================] - 0s 752us/step - loss: 24921.6803\nEpoch 199/200\n504/504 [==============================] - 0s 750us/step - loss: 24107.1960\nEpoch 200/200\n504/504 [==============================] - 0s 756us/step - loss: 24304.7270\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x2c1591c4ec8>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Make LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(75, activation=\"relu\", input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, epochs=200, verbose=1)#, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
    "y_pred = model.predict(X_test, verbose=0)\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Expected: 2120 Got: 1812 | -308\nExpected: 1831 Got: 2016 | 185\nExpected: 1961 Got: 1914 | -47\nExpected: 1813 Got: 1825 | 12\nExpected: 1809 Got: 2003 | 194\nExpected: 1974 Got: 1811 | -163\nExpected: 2005 Got: 1952 | -53\nExpected: 1948 Got: 1922 | -26\nExpected: 1921 Got: 1922 | 1\nExpected: 1858 Got: 1940 | 82\nExpected: 1852 Got: 1889 | 37\nExpected: 1918 Got: 2085 | 167\nExpected: 1999 Got: 1996 | -3\nExpected: 1750 Got: 2075 | 325\nExpected: 1845 Got: 1914 | 69\nExpected: 1807 Got: 1846 | 39\nExpected: 1752 Got: 1815 | 63\nExpected: 1782 Got: 1834 | 52\nExpected: 1839 Got: 1798 | -41\nExpected: 1709 Got: 1845 | 136\nExpected: 1853 Got: 1876 | 23\nExpected: 1798 Got: 1777 | -21\nExpected: 1871 Got: 1737 | -134\nExpected: 2038 Got: 1774 | -264\nExpected: 2287 Got: 1838 | -449\nExpected: 2304 Got: 1932 | -372\nExpected: 2231 Got: 2009 | -222\nExpected: 2056 Got: 1983 | -73\nExpected: 2066 Got: 1952 | -114\nExpected: 2043 Got: 1964 | -79\nExpected: 2063 Got: 1959 | -104\nExpected: 2068 Got: 1914 | -154\nExpected: 2123 Got: 2082 | -41\nExpected: 1974 Got: 2071 | 97\nExpected: 1899 Got: 1934 | 35\nExpected: 1883 Got: 1890 | 7\nExpected: 2017 Got: 1987 | -30\nExpected: 2012 Got: 1918 | -94\nExpected: 1940 Got: 1915 | -25\nExpected: 1875 Got: 1966 | 91\nExpected: 1895 Got: 1962 | 67\nExpected: 1818 Got: 2003 | 185\nExpected: 1769 Got: 1951 | 182\nExpected: 1691 Got: 1831 | 140\nExpected: 1742 Got: 1994 | 252\nExpected: 1702 Got: 1923 | 221\nExpected: 1735 Got: 1806 | 71\nExpected: 1983 Got: 1897 | -86\nExpected: 2289 Got: 1921 | -368\nExpected: 2257 Got: 1955 | -302\nExpected: 1929 Got: 2060 | 131\nExpected: 2016 Got: 2030 | 14\nExpected: 1965 Got: 1983 | 18\nExpected: 1956 Got: 1978 | 22\nExpected: 2066 Got: 2024 | -42\nExpected: 2030 Got: 1956 | -74\nExpected: 2052 Got: 1990 | -62\nExpected: 1942 Got: 1949 | 7\nExpected: 1970 Got: 1936 | -34\nExpected: 1981 Got: 1960 | -21\nExpected: 1899 Got: 2045 | 146\nExpected: 1859 Got: 1989 | 130\nExpected: 1946 Got: 1994 | 48\nExpected: 1864 Got: 1849 | -15\nExpected: 1777 Got: 1849 | 72\nExpected: 1762 Got: 1881 | 119\nExpected: 1754 Got: 1846 | 92\nExpected: 1717 Got: 1874 | 157\nExpected: 1688 Got: 1860 | 172\nExpected: 1731 Got: 1804 | 73\nExpected: 1708 Got: 1919 | 211\nExpected: 1946 Got: 1801 | -145\nExpected: 2174 Got: 1821 | -353\nExpected: 2104 Got: 2019 | -85\nExpected: 2053 Got: 2103 | 50\nExpected: 1942 Got: 1948 | 6\nExpected: 1761 Got: 2065 | 304\nExpected: 1962 Got: 1986 | 24\nExpected: 1982 Got: 1904 | -78\nExpected: 2072 Got: 1883 | -189\nExpected: 1922 Got: 1882 | -40\nExpected: 1965 Got: 1906 | -59\nExpected: 1732 Got: 1906 | 174\nExpected: 1789 Got: 1897 | 108\nExpected: 1797 Got: 1955 | 158\nExpected: 1733 Got: 1912 | 179\nExpected: 1758 Got: 1924 | 166\nExpected: 1734 Got: 1884 | 150\nExpected: 1679 Got: 1746 | 67\nExpected: 1714 Got: 1799 | 85\nExpected: 1671 Got: 1812 | 141\nExpected: 1655 Got: 1859 | 204\nExpected: 1711 Got: 1854 | 143\nExpected: 1721 Got: 1770 | 49\nExpected: 1776 Got: 1746 | -30\nExpected: 2118 Got: 1765 | -353\nExpected: 2171 Got: 1802 | -369\nExpected: 2065 Got: 1975 | -90\nExpected: 1924 Got: 2075 | 151\nExpected: 1951 Got: 2059 | 108\nExpected: 2154 Got: 1809 | -345\nExpected: 1929 Got: 2027 | 98\nExpected: 2045 Got: 2014 | -31\nExpected: 1989 Got: 1845 | -144\nExpected: 1953 Got: 1906 | -47\nExpected: 1928 Got: 1875 | -53\nExpected: 1942 Got: 1869 | -73\nExpected: 1908 Got: 1970 | 62\nExpected: 1818 Got: 1892 | 74\nExpected: 1842 Got: 1875 | 33\nExpected: 1772 Got: 1892 | 120\nExpected: 1730 Got: 1830 | 100\nExpected: 1888 Got: 1765 | -123\nExpected: 1792 Got: 1725 | -67\nExpected: 1776 Got: 1709 | -67\nExpected: 1763 Got: 1800 | 37\nExpected: 1720 Got: 1885 | 165\nExpected: 1620 Got: 1788 | 168\nExpected: 1724 Got: 1774 | 50\nExpected: 1979 Got: 1863 | -116\nExpected: 2178 Got: 1894 | -284\nExpected: 2169 Got: 1912 | -257\nExpected: 2022 Got: 1901 | -121\nExpected: 2075 Got: 1984 | -91\nExpected: 2356 Got: 1947 | -409\nExpected: 2020 Got: 1969 | -51\nExpected: 1979 Got: 2144 | 165\nExpected: 1948 Got: 1903 | -45\nExpected: 2016 Got: 2025 | 9\nExpected: 1908 Got: 2029 | 121\nExpected: 1952 Got: 1910 | -42\n\nTotal mean error: 119\nLargest error: 449\nSmallest error: 1\n"
    }
   ],
   "source": [
    "total_error = 0\n",
    "largest_error = None\n",
    "min_error = None\n",
    "for real, pred in zip(y_test, y_pred):\n",
    "    print(\"Expected: %d Got: %d\" % (real, pred[0]), end=\" | \")\n",
    "    error = int(pred[0]) - int(real)\n",
    "    print(error)\n",
    "    if error < 0:\n",
    "        error = error*-1\n",
    "    total_error += error\n",
    "    if largest_error == None and min_error == None:\n",
    "        largest_error, min_error = error, error\n",
    "    if error > largest_error:\n",
    "        largest_error = error\n",
    "    elif error < min_error:\n",
    "        min_error = error\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total mean error:\", str(int(total_error/len(y_test))))\n",
    "print(\"Largest error:\", largest_error)\n",
    "print(\"Smallest error:\", min_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}