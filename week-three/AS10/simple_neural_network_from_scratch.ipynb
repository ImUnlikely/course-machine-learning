{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitc4b27e4dae3c4699860ff02916ead089",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    Area  Perimeter  Compactness  Length of kernel  Width of kernel  \\\n0  14.29      14.09       0.9050             5.291            3.337   \n1  13.84      13.94       0.8955             5.324            3.379   \n2  16.14      14.99       0.9034             5.658            3.562   \n3  14.38      14.21       0.8951             5.386            3.312   \n4  14.69      14.49       0.8799             5.563            3.259   \n\n   Asymmetry coefficient  Length of kernel groove  Type  \n0                  2.699                    4.825     1  \n1                  2.259                    4.805     1  \n2                  1.355                    5.175     1  \n3                  2.462                    4.956     1  \n4                  3.586                    5.219     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Perimeter</th>\n      <th>Compactness</th>\n      <th>Length of kernel</th>\n      <th>Width of kernel</th>\n      <th>Asymmetry coefficient</th>\n      <th>Length of kernel groove</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.29</td>\n      <td>14.09</td>\n      <td>0.9050</td>\n      <td>5.291</td>\n      <td>3.337</td>\n      <td>2.699</td>\n      <td>4.825</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.84</td>\n      <td>13.94</td>\n      <td>0.8955</td>\n      <td>5.324</td>\n      <td>3.379</td>\n      <td>2.259</td>\n      <td>4.805</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.14</td>\n      <td>14.99</td>\n      <td>0.9034</td>\n      <td>5.658</td>\n      <td>3.562</td>\n      <td>1.355</td>\n      <td>5.175</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.38</td>\n      <td>14.21</td>\n      <td>0.8951</td>\n      <td>5.386</td>\n      <td>3.312</td>\n      <td>2.462</td>\n      <td>4.956</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14.69</td>\n      <td>14.49</td>\n      <td>0.8799</td>\n      <td>5.563</td>\n      <td>3.259</td>\n      <td>3.586</td>\n      <td>5.219</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 529
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Column names gotten from https://archive.ics.uci.edu/ml/datasets/seeds#)\n",
    "column_names = [\"Area\", \"Perimeter\", \"Compactness\", \"Length of kernel\", \"Width of kernel\", \"Asymmetry coefficient\", \"Length of kernel groove\", \"Type\"]\n",
    "\n",
    "df = pd.read_csv(\"seeds_dataset.csv\", header=1, names=column_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 2, 3], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 530
    }
   ],
   "source": [
    "df[\"Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    0\n1    0\n2    0\n3    0\n4    0\nName: Type, dtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                0           1           2           3           4           5  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.401830    0.443778    0.570327    0.410611    0.447605    0.384234   \nstd      0.276070    0.271096    0.215362    0.250603    0.270478    0.194463   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.156752    0.214360    0.440336    0.202703    0.220777    0.242059   \n50%      0.353636    0.387397    0.593013    0.348536    0.428724    0.369840   \n75%      0.636449    0.687500    0.724365    0.611205    0.664469    0.522813   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n                6  \ncount  208.000000  \nmean     0.439266  \nstd      0.242573  \nmin      0.000000  \n25%      0.258986  \n50%      0.348104  \n75%      0.668882  \nmax      1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.401830</td>\n      <td>0.443778</td>\n      <td>0.570327</td>\n      <td>0.410611</td>\n      <td>0.447605</td>\n      <td>0.384234</td>\n      <td>0.439266</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.276070</td>\n      <td>0.271096</td>\n      <td>0.215362</td>\n      <td>0.250603</td>\n      <td>0.270478</td>\n      <td>0.194463</td>\n      <td>0.242573</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.156752</td>\n      <td>0.214360</td>\n      <td>0.440336</td>\n      <td>0.202703</td>\n      <td>0.220777</td>\n      <td>0.242059</td>\n      <td>0.258986</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.353636</td>\n      <td>0.387397</td>\n      <td>0.593013</td>\n      <td>0.348536</td>\n      <td>0.428724</td>\n      <td>0.369840</td>\n      <td>0.348104</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.636449</td>\n      <td>0.687500</td>\n      <td>0.724365</td>\n      <td>0.611205</td>\n      <td>0.664469</td>\n      <td>0.522813</td>\n      <td>0.668882</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 531
    }
   ],
   "source": [
    "dict_to_replace = {1:np.int(0), 2:np.int(1), 3:np.int(2)}\n",
    "y = df[\"Type\"]\n",
    "y = y.replace(dict_to_replace)\n",
    "print(y[:5])\n",
    "\n",
    "X = df.drop(\"Type\", axis=1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(data=X)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.34938621 0.34710744 0.87931034 ... 0.25145302 0.1506647  0.        ]\n [0.3068933  0.3161157  0.79310345 ... 0.19424255 0.14081733 0.        ]\n [0.52407932 0.53305785 0.86479129 ... 0.07670104 0.3229936  0.        ]\n ...\n [0.24645892 0.25826446 0.7277677  ... 0.98166664 0.26440177 2.        ]\n [0.11803588 0.16528926 0.39927405 ... 0.36834441 0.25849335 2.        ]\n [0.16147309 0.19214876 0.54718693 ... 0.63346292 0.26784835 2.        ]]\n"
    }
   ],
   "source": [
    "data = pd.concat((X, y),axis=1)\n",
    "data = data.to_numpy()\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(166, 8)\n(42, 8)\n[[0.07082152974504252, 0.09504132231404938, 0.4673321234119783, 0.08671171171171155, 0.15609408410548808, 0.3357084346435398, 0.23830625307730147, 2], [0.14353163361661947, 0.17768595041322266, 0.5063520871143368, 0.18975225225225278, 0.24590163934426257, 0.4377771132117178, 0.2427375677006398, 2], [0.47025495750708224, 0.5661157024793386, 0.40471869328493604, 0.5748873873873874, 0.4283677833214543, 0.24378161203500243, 0.6696208764155585, 1]]\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train, test = tts(data, train_size=0.8, random_state=2)\n",
    "\n",
    "\n",
    "print(type(train))\n",
    "print(type(test))\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train = train.tolist()\n",
    "test = test.tolist()\n",
    "\n",
    "for row in train:\n",
    "    row[-1] = int(row[-1])\n",
    "\n",
    "for row in test:\n",
    "    row[-1] = int(row[-1])\n",
    "\n",
    "print(test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Neuron activation using sigmoid function\n",
    "# def sigmoid(x):\n",
    "#     val = 1/(1+np.exp(-x)) #maybe replace with math.exp\n",
    "#     return val\n",
    "\n",
    "# # Calculate the derivative of a neuron output\n",
    "# def sigmoid_deriv(x):\n",
    "#     return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# # Calculate neuron activation for in input\n",
    "# def activate(weights, bias, inputs):\n",
    "#     activation = bias\n",
    "#     for i in range(len(weights)):\n",
    "#         activation += weights[i] * inputs[i]\n",
    "#     return activation\n",
    "\n",
    "# # Forward propagate input to a network output\n",
    "# def forward_propagate(network, row):\n",
    "#     inputs = row\n",
    "#     for layer in network[\"HiddenLayers\"]:\n",
    "#         new_inputs = []\n",
    "#         for neuron in layer:\n",
    "#             activation = activate(neuron[\"weights\"], neuron[\"bias\"], inputs)\n",
    "#             neuron[\"output\"] = sigmoid(activation)\n",
    "#             new_inputs.append(neuron[\"output\"])\n",
    "#         inputs = new_inputs\n",
    "#     return inputs\n",
    "\n",
    "# # Backpropagate error and store in neurons\n",
    "# def backward_propagate_error(network, expected):\n",
    "#     output_layer_complete = False\n",
    "#     for i in reversed(range(len(network[\"HiddenLayers\"])+1)):\n",
    "#         output_layer = network[\"Output\"]\n",
    "#         if i != len(network[\"HiddenLayers\"]):\n",
    "#             layer = network[\"HiddenLayers\"][i]\n",
    "#         errors = list()\n",
    "#         #if i != len(network[\"HiddenLayers\"])-1:\n",
    "#         if output_layer_complete == True:\n",
    "#             if i == len(network[\"HiddenLayers\"]):\n",
    "#                 for j in range(len(network[\"Output\"])):\n",
    "#                     error = 0.0\n",
    "#                     for neuron in network[\"Output\"]:\n",
    "#                         error += (neuron[\"weights\"][j] * neuron[\"delta\"])\n",
    "#                         errors.append(error)\n",
    "#             else:\n",
    "#                 for j in range(len(layer)):\n",
    "#                     error = 0.0\n",
    "#                     for neuron in network[\"HiddenLayers\"][i]:\n",
    "#                         error += (neuron[\"weights\"][j] * neuron[\"delta\"])\n",
    "#                         errors.append(error)                \n",
    "#         else:\n",
    "#             for j in range(len(output_layer)):\n",
    "#                 neuron = output_layer[j]\n",
    "#                 errors.append(expected[j] - neuron[\"output\"])\n",
    "#                 output_layer_complete = True\n",
    "#         for j in range(len(layer)):\n",
    "#             neuron = layer[j]\n",
    "#             neuron[\"delta\"] = errors[j] * sigmoid_deriv(neuron[\"output\"])\n",
    "\n",
    "# # Update weights with error\n",
    "# def update_weights(network, row, l_rate):\n",
    "#     for i in range(len(network[\"HiddenLayers\"])):\n",
    "#         inputs = row[:-1]\n",
    "#         if i != 0:\n",
    "#             inputs = [neuron[\"output\"] for neuron in network[\"HiddenLayers\"][i - 1]]\n",
    "#         for neuron in network[\"HiddenLayers\"][i]:\n",
    "#             for j in range(len(inputs)):\n",
    "#                 neuron[\"weights\"][j] += l_rate * neuron[\"delta\"] * inputs[j]\n",
    "#             neuron[\"bias\"] += l_rate * neuron[\"delta\"]\n",
    "\n",
    "# # Train a network for a fixed number of epochs\n",
    "# def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "#     for epoch in range(n_epoch):\n",
    "#         sum_error = 0\n",
    "#         for row in train:\n",
    "#             outputs = forward_propagate(network, row)\n",
    "#             expected = [0 for i in range(n_outputs)]\n",
    "#             expected[int(row[-1])] = 1\n",
    "#             sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "#             backward_propagate_error(network, expected)\n",
    "#             update_weights(network, row, l_rate)\n",
    "#         print(f'>epoch={epoch}, lrate={l_rate}, error={np.mean(sum_error)}')\n",
    "\n",
    "# # Make a prediction with a network\n",
    "# def predict(network, row):\n",
    "#     outputs = forward_propagate(network, row)\n",
    "#     for smn in outputs:\n",
    "#         print(smn)\n",
    "#     return outputs.index(max(outputs))\n",
    "\n",
    "# # Calculate accuracy\n",
    "# def accuracy_metric(actual, predicted):\n",
    "#     correct = 0\n",
    "#     for i in range(len(actual)):\n",
    "#         if actual[i] == predicted[i]:\n",
    "#             correct += 1\n",
    "#     return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# # Backpropagation Algorithm with stochastic gradient descent\n",
    "# def back_propagation(train, l_rate, n_epoch, n_hidden):\n",
    "#     n_inputs = len(train[0]) - 1\n",
    "#     n_outputs = len(set([row[-1] for row in train]))\n",
    "#     network = create_network(n_inputs, 1, [n_hidden], n_outputs)\n",
    "#     train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "#     return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]            # to get the bias. If you dont have bias then set the activation to zero\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]  # sum up the product of the weight and input value\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation using sigmoid function\n",
    "from math import exp\n",
    "def transfer(activation): # SIGMOID IN MY NOTEBOOK\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "# Forward-propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    "\n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "        \n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def back_propagation(train, l_rate, n_epoch, n_hidden):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "import json\n",
    "\n",
    "def print_json_dump(dump):\n",
    "    print(json.dumps(dump, sort_keys=True, indent=4))\n",
    "\n",
    "# generate weights and bias\n",
    "def create_network(n_inputs:int, n_hidden_layers:int, n_nodes_for_layer:list, n_outputs:int):\n",
    "    \"\"\"Creates a neural network with layers, neurons with weights and bias, output neurons with weights and bias\n",
    "\n",
    "    Args:\n",
    "        n_inputs (int): The amount of input features\n",
    "        n_hidden_layers (int): The amount of hidden layers desired\n",
    "        n_nodes_for_layer (list): A list containing the number of neurons per hidden layer\n",
    "        n_outputs (int): Amount of output neurons wanted\n",
    "\n",
    "    Returns:\n",
    "        (dict): Your neural network\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    assert len(n_nodes_for_layer) == n_hidden_layers, \\\n",
    "        (\"The length of this list needs to be the same as n_hidden_layers\")\n",
    "\n",
    "    network = {\"HiddenLayers\":[], \"Output\":[]}\n",
    "    current_layer = -1\n",
    "\n",
    "    for hidden_layer in range(n_hidden_layers):\n",
    "        current_layer += 1\n",
    "        layer = []\n",
    "\n",
    "        for nodes in range(n_nodes_for_layer[current_layer]):\n",
    "\n",
    "            if current_layer == 0:\n",
    "                weights = [random() for i in range(n_inputs)]\n",
    "            elif current_layer > 0:\n",
    "                weights = [random() for i in range(n_nodes_for_layer[current_layer-1])]\n",
    "\n",
    "            bias = random()\n",
    "            node = {\"weights\":weights, \"bias\":bias}\n",
    "            layer.append(node)\n",
    "\n",
    "        network[\"HiddenLayers\"].append(layer)\n",
    "\n",
    "    \n",
    "    n_output_weights = len(network[\"HiddenLayers\"][-1])\n",
    "    for i in range(n_outputs):\n",
    "\n",
    "        weights = [random() for k in range(n_output_weights)]\n",
    "        bias = random()\n",
    "        node = {\"weights\":weights, \"bias\":bias}\n",
    "        network[\"Output\"].append(node)\n",
    "\n",
    "    # print_json_dump(network)\n",
    "\n",
    "    return network\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "seed(1)\n",
    "my_net = create_network(3, 1, [3], 3)\n",
    "# my_net = initialize_network(7, 10, 3)\n",
    "# print(my_net)\n",
    "# print(print_json_dump(my_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " lrate=0.100, error=11.565\n>epoch=485, lrate=0.100, error=11.552\n>epoch=486, lrate=0.100, error=11.539\n>epoch=487, lrate=0.100, error=11.527\n>epoch=488, lrate=0.100, error=11.514\n>epoch=489, lrate=0.100, error=11.501\n>epoch=490, lrate=0.100, error=11.488\n>epoch=491, lrate=0.100, error=11.476\n>epoch=492, lrate=0.100, error=11.463\n>epoch=493, lrate=0.100, error=11.451\n>epoch=494, lrate=0.100, error=11.438\n>epoch=495, lrate=0.100, error=11.426\n>epoch=496, lrate=0.100, error=11.413\n>epoch=497, lrate=0.100, error=11.401\n>epoch=498, lrate=0.100, error=11.388\n>epoch=499, lrate=0.100, error=11.376\n>epoch=500, lrate=0.100, error=11.364\n>epoch=501, lrate=0.100, error=11.351\n>epoch=502, lrate=0.100, error=11.339\n>epoch=503, lrate=0.100, error=11.327\n>epoch=504, lrate=0.100, error=11.315\n>epoch=505, lrate=0.100, error=11.302\n>epoch=506, lrate=0.100, error=11.290\n>epoch=507, lrate=0.100, error=11.278\n>epoch=508, lrate=0.100, error=11.266\n>epoch=509, lrate=0.100, error=11.254\n>epoch=510, lrate=0.100, error=11.242\n>epoch=511, lrate=0.100, error=11.230\n>epoch=512, lrate=0.100, error=11.218\n>epoch=513, lrate=0.100, error=11.206\n>epoch=514, lrate=0.100, error=11.194\n>epoch=515, lrate=0.100, error=11.182\n>epoch=516, lrate=0.100, error=11.170\n>epoch=517, lrate=0.100, error=11.159\n>epoch=518, lrate=0.100, error=11.147\n>epoch=519, lrate=0.100, error=11.135\n>epoch=520, lrate=0.100, error=11.123\n>epoch=521, lrate=0.100, error=11.112\n>epoch=522, lrate=0.100, error=11.100\n>epoch=523, lrate=0.100, error=11.088\n>epoch=524, lrate=0.100, error=11.077\n>epoch=525, lrate=0.100, error=11.065\n>epoch=526, lrate=0.100, error=11.054\n>epoch=527, lrate=0.100, error=11.042\n>epoch=528, lrate=0.100, error=11.031\n>epoch=529, lrate=0.100, error=11.019\n>epoch=530, lrate=0.100, error=11.008\n>epoch=531, lrate=0.100, error=10.996\n>epoch=532, lrate=0.100, error=10.985\n>epoch=533, lrate=0.100, error=10.974\n>epoch=534, lrate=0.100, error=10.962\n>epoch=535, lrate=0.100, error=10.951\n>epoch=536, lrate=0.100, error=10.940\n>epoch=537, lrate=0.100, error=10.928\n>epoch=538, lrate=0.100, error=10.917\n>epoch=539, lrate=0.100, error=10.906\n>epoch=540, lrate=0.100, error=10.895\n>epoch=541, lrate=0.100, error=10.884\n>epoch=542, lrate=0.100, error=10.873\n>epoch=543, lrate=0.100, error=10.862\n>epoch=544, lrate=0.100, error=10.851\n>epoch=545, lrate=0.100, error=10.839\n>epoch=546, lrate=0.100, error=10.829\n>epoch=547, lrate=0.100, error=10.818\n>epoch=548, lrate=0.100, error=10.807\n>epoch=549, lrate=0.100, error=10.796\n>epoch=550, lrate=0.100, error=10.785\n>epoch=551, lrate=0.100, error=10.774\n>epoch=552, lrate=0.100, error=10.763\n>epoch=553, lrate=0.100, error=10.752\n>epoch=554, lrate=0.100, error=10.741\n>epoch=555, lrate=0.100, error=10.731\n>epoch=556, lrate=0.100, error=10.720\n>epoch=557, lrate=0.100, error=10.709\n>epoch=558, lrate=0.100, error=10.699\n>epoch=559, lrate=0.100, error=10.688\n>epoch=560, lrate=0.100, error=10.677\n>epoch=561, lrate=0.100, error=10.667\n>epoch=562, lrate=0.100, error=10.656\n>epoch=563, lrate=0.100, error=10.645\n>epoch=564, lrate=0.100, error=10.635\n>epoch=565, lrate=0.100, error=10.624\n>epoch=566, lrate=0.100, error=10.614\n>epoch=567, lrate=0.100, error=10.603\n>epoch=568, lrate=0.100, error=10.593\n>epoch=569, lrate=0.100, error=10.583\n>epoch=570, lrate=0.100, error=10.572\n>epoch=571, lrate=0.100, error=10.562\n>epoch=572, lrate=0.100, error=10.552\n>epoch=573, lrate=0.100, error=10.541\n>epoch=574, lrate=0.100, error=10.531\n>epoch=575, lrate=0.100, error=10.521\n>epoch=576, lrate=0.100, error=10.510\n>epoch=577, lrate=0.100, error=10.500\n>epoch=578, lrate=0.100, error=10.490\n>epoch=579, lrate=0.100, error=10.480\n>epoch=580, lrate=0.100, error=10.470\n>epoch=581, lrate=0.100, error=10.459\n>epoch=582, lrate=0.100, error=10.449\n>epoch=583, lrate=0.100, error=10.439\n>epoch=584, lrate=0.100, error=10.429\n>epoch=585, lrate=0.100, error=10.419\n>epoch=586, lrate=0.100, error=10.409\n>epoch=587, lrate=0.100, error=10.399\n>epoch=588, lrate=0.100, error=10.389\n>epoch=589, lrate=0.100, error=10.379\n>epoch=590, lrate=0.100, error=10.369\n>epoch=591, lrate=0.100, error=10.359\n>epoch=592, lrate=0.100, error=10.350\n>epoch=593, lrate=0.100, error=10.340\n>epoch=594, lrate=0.100, error=10.330\n>epoch=595, lrate=0.100, error=10.320\n>epoch=596, lrate=0.100, error=10.310\n>epoch=597, lrate=0.100, error=10.300\n>epoch=598, lrate=0.100, error=10.291\n>epoch=599, lrate=0.100, error=10.281\n>epoch=600, lrate=0.100, error=10.271\n>epoch=601, lrate=0.100, error=10.262\n>epoch=602, lrate=0.100, error=10.252\n>epoch=603, lrate=0.100, error=10.242\n>epoch=604, lrate=0.100, error=10.233\n>epoch=605, lrate=0.100, error=10.223\n>epoch=606, lrate=0.100, error=10.213\n>epoch=607, lrate=0.100, error=10.204\n>epoch=608, lrate=0.100, error=10.194\n>epoch=609, lrate=0.100, error=10.185\n>epoch=610, lrate=0.100, error=10.175\n>epoch=611, lrate=0.100, error=10.166\n>epoch=612, lrate=0.100, error=10.156\n>epoch=613, lrate=0.100, error=10.147\n>epoch=614, lrate=0.100, error=10.137\n>epoch=615, lrate=0.100, error=10.128\n>epoch=616, lrate=0.100, error=10.119\n>epoch=617, lrate=0.100, error=10.109\n>epoch=618, lrate=0.100, error=10.100\n>epoch=619, lrate=0.100, error=10.091\n>epoch=620, lrate=0.100, error=10.081\n>epoch=621, lrate=0.100, error=10.072\n>epoch=622, lrate=0.100, error=10.063\n>epoch=623, lrate=0.100, error=10.054\n>epoch=624, lrate=0.100, error=10.044\n>epoch=625, lrate=0.100, error=10.035\n>epoch=626, lrate=0.100, error=10.026\n>epoch=627, lrate=0.100, error=10.017\n>epoch=628, lrate=0.100, error=10.008\n>epoch=629, lrate=0.100, error=9.999\n>epoch=630, lrate=0.100, error=9.989\n>epoch=631, lrate=0.100, error=9.980\n>epoch=632, lrate=0.100, error=9.971\n>epoch=633, lrate=0.100, error=9.962\n>epoch=634, lrate=0.100, error=9.953\n>epoch=635, lrate=0.100, error=9.944\n>epoch=636, lrate=0.100, error=9.935\n>epoch=637, lrate=0.100, error=9.926\n>epoch=638, lrate=0.100, error=9.917\n>epoch=639, lrate=0.100, error=9.908\n>epoch=640, lrate=0.100, error=9.899\n>epoch=641, lrate=0.100, error=9.890\n>epoch=642, lrate=0.100, error=9.882\n>epoch=643, lrate=0.100, error=9.873\n>epoch=644, lrate=0.100, error=9.864\n>epoch=645, lrate=0.100, error=9.855\n>epoch=646, lrate=0.100, error=9.846\n>epoch=647, lrate=0.100, error=9.837\n>epoch=648, lrate=0.100, error=9.829\n>epoch=649, lrate=0.100, error=9.820\n>epoch=650, lrate=0.100, error=9.811\n>epoch=651, lrate=0.100, error=9.802\n>epoch=652, lrate=0.100, error=9.794\n>epoch=653, lrate=0.100, error=9.785\n>epoch=654, lrate=0.100, error=9.776\n>epoch=655, lrate=0.100, error=9.768\n>epoch=656, lrate=0.100, error=9.759\n>epoch=657, lrate=0.100, error=9.750\n>epoch=658, lrate=0.100, error=9.742\n>epoch=659, lrate=0.100, error=9.733\n>epoch=660, lrate=0.100, error=9.725\n>epoch=661, lrate=0.100, error=9.716\n>epoch=662, lrate=0.100, error=9.707\n>epoch=663, lrate=0.100, error=9.699\n>epoch=664, lrate=0.100, error=9.690\n>epoch=665, lrate=0.100, error=9.682\n>epoch=666, lrate=0.100, error=9.673\n>epoch=667, lrate=0.100, error=9.665\n>epoch=668, lrate=0.100, error=9.657\n>epoch=669, lrate=0.100, error=9.648\n>epoch=670, lrate=0.100, error=9.640\n>epoch=671, lrate=0.100, error=9.631\n>epoch=672, lrate=0.100, error=9.623\n>epoch=673, lrate=0.100, error=9.615\n>epoch=674, lrate=0.100, error=9.606\n>epoch=675, lrate=0.100, error=9.598\n>epoch=676, lrate=0.100, error=9.590\n>epoch=677, lrate=0.100, error=9.581\n>epoch=678, lrate=0.100, error=9.573\n>epoch=679, lrate=0.100, error=9.565\n>epoch=680, lrate=0.100, error=9.556\n>epoch=681, lrate=0.100, error=9.548\n>epoch=682, lrate=0.100, error=9.540\n>epoch=683, lrate=0.100, error=9.532\n>epoch=684, lrate=0.100, error=9.524\n>epoch=685, lrate=0.100, error=9.515\n>epoch=686, lrate=0.100, error=9.507\n>epoch=687, lrate=0.100, error=9.499\n>epoch=688, lrate=0.100, error=9.491\n>epoch=689, lrate=0.100, error=9.483\n>epoch=690, lrate=0.100, error=9.475\n>epoch=691, lrate=0.100, error=9.467\n>epoch=692, lrate=0.100, error=9.459\n>epoch=693, lrate=0.100, error=9.451\n>epoch=694, lrate=0.100, error=9.442\n>epoch=695, lrate=0.100, error=9.434\n>epoch=696, lrate=0.100, error=9.426\n>epoch=697, lrate=0.100, error=9.418\n>epoch=698, lrate=0.100, error=9.410\n>epoch=699, lrate=0.100, error=9.402\n>epoch=700, lrate=0.100, error=9.395\n>epoch=701, lrate=0.100, error=9.387\n>epoch=702, lrate=0.100, error=9.379\n>epoch=703, lrate=0.100, error=9.371\n>epoch=704, lrate=0.100, error=9.363\n>epoch=705, lrate=0.100, error=9.355\n>epoch=706, lrate=0.100, error=9.347\n>epoch=707, lrate=0.100, error=9.339\n>epoch=708, lrate=0.100, error=9.331\n>epoch=709, lrate=0.100, error=9.324\n>epoch=710, lrate=0.100, error=9.316\n>epoch=711, lrate=0.100, error=9.308\n>epoch=712, lrate=0.100, error=9.300\n>epoch=713, lrate=0.100, error=9.292\n>epoch=714, lrate=0.100, error=9.285\n>epoch=715, lrate=0.100, error=9.277\n>epoch=716, lrate=0.100, error=9.269\n>epoch=717, lrate=0.100, error=9.261\n>epoch=718, lrate=0.100, error=9.254\n>epoch=719, lrate=0.100, error=9.246\n>epoch=720, lrate=0.100, error=9.238\n>epoch=721, lrate=0.100, error=9.231\n>epoch=722, lrate=0.100, error=9.223\n>epoch=723, lrate=0.100, error=9.215\n>epoch=724, lrate=0.100, error=9.208\n>epoch=725, lrate=0.100, error=9.200\n>epoch=726, lrate=0.100, error=9.192\n>epoch=727, lrate=0.100, error=9.185\n>epoch=728, lrate=0.100, error=9.177\n>epoch=729, lrate=0.100, error=9.170\n>epoch=730, lrate=0.100, error=9.162\n>epoch=731, lrate=0.100, error=9.155\n>epoch=732, lrate=0.100, error=9.147\n>epoch=733, lrate=0.100, error=9.139\n>epoch=734, lrate=0.100, error=9.132\n>epoch=735, lrate=0.100, error=9.124\n>epoch=736, lrate=0.100, error=9.117\n>epoch=737, lrate=0.100, error=9.109\n>epoch=738, lrate=0.100, error=9.102\n>epoch=739, lrate=0.100, error=9.095\n>epoch=740, lrate=0.100, error=9.087\n>epoch=741, lrate=0.100, error=9.080\n>epoch=742, lrate=0.100, error=9.072\n>epoch=743, lrate=0.100, error=9.065\n>epoch=744, lrate=0.100, error=9.057\n>epoch=745, lrate=0.100, error=9.050\n>epoch=746, lrate=0.100, error=9.043\n>epoch=747, lrate=0.100, error=9.035\n>epoch=748, lrate=0.100, error=9.028\n>epoch=749, lrate=0.100, error=9.021\n>epoch=750, lrate=0.100, error=9.013\n>epoch=751, lrate=0.100, error=9.006\n>epoch=752, lrate=0.100, error=8.999\n>epoch=753, lrate=0.100, error=8.991\n>epoch=754, lrate=0.100, error=8.984\n>epoch=755, lrate=0.100, error=8.977\n>epoch=756, lrate=0.100, error=8.970\n>epoch=757, lrate=0.100, error=8.962\n>epoch=758, lrate=0.100, error=8.955\n>epoch=759, lrate=0.100, error=8.948\n>epoch=760, lrate=0.100, error=8.941\n>epoch=761, lrate=0.100, error=8.934\n>epoch=762, lrate=0.100, error=8.926\n>epoch=763, lrate=0.100, error=8.919\n>epoch=764, lrate=0.100, error=8.912\n>epoch=765, lrate=0.100, error=8.905\n>epoch=766, lrate=0.100, error=8.898\n>epoch=767, lrate=0.100, error=8.891\n>epoch=768, lrate=0.100, error=8.883\n>epoch=769, lrate=0.100, error=8.876\n>epoch=770, lrate=0.100, error=8.869\n>epoch=771, lrate=0.100, error=8.862\n>epoch=772, lrate=0.100, error=8.855\n>epoch=773, lrate=0.100, error=8.848\n>epoch=774, lrate=0.100, error=8.841\n>epoch=775, lrate=0.100, error=8.834\n>epoch=776, lrate=0.100, error=8.827\n>epoch=777, lrate=0.100, error=8.820\n>epoch=778, lrate=0.100, error=8.813\n>epoch=779, lrate=0.100, error=8.806\n>epoch=780, lrate=0.100, error=8.799\n>epoch=781, lrate=0.100, error=8.792\n>epoch=782, lrate=0.100, error=8.785\n>epoch=783, lrate=0.100, error=8.778\n>epoch=784, lrate=0.100, error=8.771\n>epoch=785, lrate=0.100, error=8.764\n>epoch=786, lrate=0.100, error=8.757\n>epoch=787, lrate=0.100, error=8.750\n>epoch=788, lrate=0.100, error=8.743\n>epoch=789, lrate=0.100, error=8.736\n>epoch=790, lrate=0.100, error=8.729\n>epoch=791, lrate=0.100, error=8.722\n>epoch=792, lrate=0.100, error=8.716\n>epoch=793, lrate=0.100, error=8.709\n>epoch=794, lrate=0.100, error=8.702\n>epoch=795, lrate=0.100, error=8.695\n>epoch=796, lrate=0.100, error=8.688\n>epoch=797, lrate=0.100, error=8.681\n>epoch=798, lrate=0.100, error=8.674\n>epoch=799, lrate=0.100, error=8.668\n>epoch=800, lrate=0.100, error=8.661\n>epoch=801, lrate=0.100, error=8.654\n>epoch=802, lrate=0.100, error=8.647\n>epoch=803, lrate=0.100, error=8.640\n>epoch=804, lrate=0.100, error=8.634\n>epoch=805, lrate=0.100, error=8.627\n>epoch=806, lrate=0.100, error=8.620\n>epoch=807, lrate=0.100, error=8.613\n>epoch=808, lrate=0.100, error=8.607\n>epoch=809, lrate=0.100, error=8.600\n>epoch=810, lrate=0.100, error=8.593\n>epoch=811, lrate=0.100, error=8.587\n>epoch=812, lrate=0.100, error=8.580\n>epoch=813, lrate=0.100, error=8.573\n>epoch=814, lrate=0.100, error=8.566\n>epoch=815, lrate=0.100, error=8.560\n>epoch=816, lrate=0.100, error=8.553\n>epoch=817, lrate=0.100, error=8.546\n>epoch=818, lrate=0.100, error=8.540\n>epoch=819, lrate=0.100, error=8.533\n>epoch=820, lrate=0.100, error=8.527\n>epoch=821, lrate=0.100, error=8.520\n>epoch=822, lrate=0.100, error=8.513\n>epoch=823, lrate=0.100, error=8.507\n>epoch=824, lrate=0.100, error=8.500\n>epoch=825, lrate=0.100, error=8.493\n>epoch=826, lrate=0.100, error=8.487\n>epoch=827, lrate=0.100, error=8.480\n>epoch=828, lrate=0.100, error=8.474\n>epoch=829, lrate=0.100, error=8.467\n>epoch=830, lrate=0.100, error=8.461\n>epoch=831, lrate=0.100, error=8.454\n>epoch=832, lrate=0.100, error=8.448\n>epoch=833, lrate=0.100, error=8.441\n>epoch=834, lrate=0.100, error=8.434\n>epoch=835, lrate=0.100, error=8.428\n>epoch=836, lrate=0.100, error=8.421\n>epoch=837, lrate=0.100, error=8.415\n>epoch=838, lrate=0.100, error=8.408\n>epoch=839, lrate=0.100, error=8.402\n>epoch=840, lrate=0.100, error=8.395\n>epoch=841, lrate=0.100, error=8.389\n>epoch=842, lrate=0.100, error=8.383\n>epoch=843, lrate=0.100, error=8.376\n>epoch=844, lrate=0.100, error=8.370\n>epoch=845, lrate=0.100, error=8.363\n>epoch=846, lrate=0.100, error=8.357\n>epoch=847, lrate=0.100, error=8.350\n>epoch=848, lrate=0.100, error=8.344\n>epoch=849, lrate=0.100, error=8.338\n>epoch=850, lrate=0.100, error=8.331\n>epoch=851, lrate=0.100, error=8.325\n>epoch=852, lrate=0.100, error=8.318\n>epoch=853, lrate=0.100, error=8.312\n>epoch=854, lrate=0.100, error=8.306\n>epoch=855, lrate=0.100, error=8.299\n>epoch=856, lrate=0.100, error=8.293\n>epoch=857, lrate=0.100, error=8.286\n>epoch=858, lrate=0.100, error=8.280\n>epoch=859, lrate=0.100, error=8.274\n>epoch=860, lrate=0.100, error=8.267\n>epoch=861, lrate=0.100, error=8.261\n>epoch=862, lrate=0.100, error=8.255\n>epoch=863, lrate=0.100, error=8.248\n>epoch=864, lrate=0.100, error=8.242\n>epoch=865, lrate=0.100, error=8.236\n>epoch=866, lrate=0.100, error=8.230\n>epoch=867, lrate=0.100, error=8.223\n>epoch=868, lrate=0.100, error=8.217\n>epoch=869, lrate=0.100, error=8.211\n>epoch=870, lrate=0.100, error=8.204\n>epoch=871, lrate=0.100, error=8.198\n>epoch=872, lrate=0.100, error=8.192\n>epoch=873, lrate=0.100, error=8.186\n>epoch=874, lrate=0.100, error=8.179\n>epoch=875, lrate=0.100, error=8.173\n>epoch=876, lrate=0.100, error=8.167\n>epoch=877, lrate=0.100, error=8.161\n>epoch=878, lrate=0.100, error=8.154\n>epoch=879, lrate=0.100, error=8.148\n>epoch=880, lrate=0.100, error=8.142\n>epoch=881, lrate=0.100, error=8.136\n>epoch=882, lrate=0.100, error=8.130\n>epoch=883, lrate=0.100, error=8.123\n>epoch=884, lrate=0.100, error=8.117\n>epoch=885, lrate=0.100, error=8.111\n>epoch=886, lrate=0.100, error=8.105\n>epoch=887, lrate=0.100, error=8.099\n>epoch=888, lrate=0.100, error=8.093\n>epoch=889, lrate=0.100, error=8.086\n>epoch=890, lrate=0.100, error=8.080\n>epoch=891, lrate=0.100, error=8.074\n>epoch=892, lrate=0.100, error=8.068\n>epoch=893, lrate=0.100, error=8.062\n>epoch=894, lrate=0.100, error=8.056\n>epoch=895, lrate=0.100, error=8.050\n>epoch=896, lrate=0.100, error=8.043\n>epoch=897, lrate=0.100, error=8.037\n>epoch=898, lrate=0.100, error=8.031\n>epoch=899, lrate=0.100, error=8.025\n>epoch=900, lrate=0.100, error=8.019\n>epoch=901, lrate=0.100, error=8.013\n>epoch=902, lrate=0.100, error=8.007\n>epoch=903, lrate=0.100, error=8.001\n>epoch=904, lrate=0.100, error=7.995\n>epoch=905, lrate=0.100, error=7.989\n>epoch=906, lrate=0.100, error=7.983\n>epoch=907, lrate=0.100, error=7.977\n>epoch=908, lrate=0.100, error=7.970\n>epoch=909, lrate=0.100, error=7.964\n>epoch=910, lrate=0.100, error=7.958\n>epoch=911, lrate=0.100, error=7.952\n>epoch=912, lrate=0.100, error=7.946\n>epoch=913, lrate=0.100, error=7.940\n>epoch=914, lrate=0.100, error=7.934\n>epoch=915, lrate=0.100, error=7.928\n>epoch=916, lrate=0.100, error=7.922\n>epoch=917, lrate=0.100, error=7.916\n>epoch=918, lrate=0.100, error=7.910\n>epoch=919, lrate=0.100, error=7.904\n>epoch=920, lrate=0.100, error=7.898\n>epoch=921, lrate=0.100, error=7.892\n>epoch=922, lrate=0.100, error=7.886\n>epoch=923, lrate=0.100, error=7.880\n>epoch=924, lrate=0.100, error=7.874\n>epoch=925, lrate=0.100, error=7.868\n>epoch=926, lrate=0.100, error=7.862\n>epoch=927, lrate=0.100, error=7.856\n>epoch=928, lrate=0.100, error=7.850\n>epoch=929, lrate=0.100, error=7.845\n>epoch=930, lrate=0.100, error=7.839\n>epoch=931, lrate=0.100, error=7.833\n>epoch=932, lrate=0.100, error=7.827\n>epoch=933, lrate=0.100, error=7.821\n>epoch=934, lrate=0.100, error=7.815\n>epoch=935, lrate=0.100, error=7.809\n>epoch=936, lrate=0.100, error=7.803\n>epoch=937, lrate=0.100, error=7.797\n>epoch=938, lrate=0.100, error=7.791\n>epoch=939, lrate=0.100, error=7.785\n>epoch=940, lrate=0.100, error=7.779\n>epoch=941, lrate=0.100, error=7.773\n>epoch=942, lrate=0.100, error=7.768\n>epoch=943, lrate=0.100, error=7.762\n>epoch=944, lrate=0.100, error=7.756\n>epoch=945, lrate=0.100, error=7.750\n>epoch=946, lrate=0.100, error=7.744\n>epoch=947, lrate=0.100, error=7.738\n>epoch=948, lrate=0.100, error=7.732\n>epoch=949, lrate=0.100, error=7.726\n>epoch=950, lrate=0.100, error=7.721\n>epoch=951, lrate=0.100, error=7.715\n>epoch=952, lrate=0.100, error=7.709\n>epoch=953, lrate=0.100, error=7.703\n>epoch=954, lrate=0.100, error=7.697\n>epoch=955, lrate=0.100, error=7.691\n>epoch=956, lrate=0.100, error=7.685\n>epoch=957, lrate=0.100, error=7.680\n>epoch=958, lrate=0.100, error=7.674\n>epoch=959, lrate=0.100, error=7.668\n>epoch=960, lrate=0.100, error=7.662\n>epoch=961, lrate=0.100, error=7.656\n>epoch=962, lrate=0.100, error=7.651\n>epoch=963, lrate=0.100, error=7.645\n>epoch=964, lrate=0.100, error=7.639\n>epoch=965, lrate=0.100, error=7.633\n>epoch=966, lrate=0.100, error=7.627\n>epoch=967, lrate=0.100, error=7.621\n>epoch=968, lrate=0.100, error=7.616\n>epoch=969, lrate=0.100, error=7.610\n>epoch=970, lrate=0.100, error=7.604\n>epoch=971, lrate=0.100, error=7.598\n>epoch=972, lrate=0.100, error=7.593\n>epoch=973, lrate=0.100, error=7.587\n>epoch=974, lrate=0.100, error=7.581\n>epoch=975, lrate=0.100, error=7.575\n>epoch=976, lrate=0.100, error=7.569\n>epoch=977, lrate=0.100, error=7.564\n>epoch=978, lrate=0.100, error=7.558\n>epoch=979, lrate=0.100, error=7.552\n>epoch=980, lrate=0.100, error=7.546\n>epoch=981, lrate=0.100, error=7.541\n>epoch=982, lrate=0.100, error=7.535\n>epoch=983, lrate=0.100, error=7.529\n>epoch=984, lrate=0.100, error=7.523\n>epoch=985, lrate=0.100, error=7.518\n>epoch=986, lrate=0.100, error=7.512\n>epoch=987, lrate=0.100, error=7.506\n>epoch=988, lrate=0.100, error=7.500\n>epoch=989, lrate=0.100, error=7.495\n>epoch=990, lrate=0.100, error=7.489\n>epoch=991, lrate=0.100, error=7.483\n>epoch=992, lrate=0.100, error=7.478\n>epoch=993, lrate=0.100, error=7.472\n>epoch=994, lrate=0.100, error=7.466\n>epoch=995, lrate=0.100, error=7.460\n>epoch=996, lrate=0.100, error=7.455\n>epoch=997, lrate=0.100, error=7.449\n>epoch=998, lrate=0.100, error=7.443\n>epoch=999, lrate=0.100, error=7.438\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=1\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=0, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=2, Got=2\n95.23809523809523\n"
    }
   ],
   "source": [
    "l_rate = 0.1\n",
    "n_epoch = 1000\n",
    "n_hidden = 20\n",
    "\n",
    "# print(train[:5])\n",
    "\n",
    "# Train model\n",
    "Model = back_propagation(train, l_rate, n_epoch, n_hidden)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "PredClass = list()\n",
    "ActualClass = list()\n",
    "for row in test:\n",
    "    prediction = predict(Model, row)\n",
    "    PredClass.append(prediction)\n",
    "    ActualClass.append(row[-1])\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "accuracy = accuracy_metric(ActualClass, PredClass)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3\n2\n1\n0\n"
    }
   ],
   "source": [
    "alist = [1, 2, 3, 4]\n",
    "\n",
    "for i in reversed(range(len(alist))):\n",
    "    print(i)"
   ]
  }
 ]
}