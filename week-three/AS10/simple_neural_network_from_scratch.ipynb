{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitc4b27e4dae3c4699860ff02916ead089",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    Area  Perimeter  Compactness  Length of kernel  Width of kernel  \\\n0  14.29      14.09       0.9050             5.291            3.337   \n1  13.84      13.94       0.8955             5.324            3.379   \n2  16.14      14.99       0.9034             5.658            3.562   \n3  14.38      14.21       0.8951             5.386            3.312   \n4  14.69      14.49       0.8799             5.563            3.259   \n\n   Asymmetry coefficient  Length of kernel groove  Type  \n0                  2.699                    4.825     1  \n1                  2.259                    4.805     1  \n2                  1.355                    5.175     1  \n3                  2.462                    4.956     1  \n4                  3.586                    5.219     1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Area</th>\n      <th>Perimeter</th>\n      <th>Compactness</th>\n      <th>Length of kernel</th>\n      <th>Width of kernel</th>\n      <th>Asymmetry coefficient</th>\n      <th>Length of kernel groove</th>\n      <th>Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.29</td>\n      <td>14.09</td>\n      <td>0.9050</td>\n      <td>5.291</td>\n      <td>3.337</td>\n      <td>2.699</td>\n      <td>4.825</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13.84</td>\n      <td>13.94</td>\n      <td>0.8955</td>\n      <td>5.324</td>\n      <td>3.379</td>\n      <td>2.259</td>\n      <td>4.805</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.14</td>\n      <td>14.99</td>\n      <td>0.9034</td>\n      <td>5.658</td>\n      <td>3.562</td>\n      <td>1.355</td>\n      <td>5.175</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.38</td>\n      <td>14.21</td>\n      <td>0.8951</td>\n      <td>5.386</td>\n      <td>3.312</td>\n      <td>2.462</td>\n      <td>4.956</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14.69</td>\n      <td>14.49</td>\n      <td>0.8799</td>\n      <td>5.563</td>\n      <td>3.259</td>\n      <td>3.586</td>\n      <td>5.219</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 580
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Column names gotten from https://archive.ics.uci.edu/ml/datasets/seeds#)\n",
    "column_names = [\"Area\", \"Perimeter\", \"Compactness\", \"Length of kernel\", \"Width of kernel\", \"Asymmetry coefficient\", \"Length of kernel groove\", \"Type\"]\n",
    "\n",
    "df = pd.read_csv(\"seeds_dataset.csv\", header=1, names=column_names)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([1, 2, 3], dtype=int64)"
     },
     "metadata": {},
     "execution_count": 581
    }
   ],
   "source": [
    "df[\"Type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0    0\n1    0\n2    0\n3    0\n4    0\nName: Type, dtype: int64\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                0           1           2           3           4           5  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.401830    0.443778    0.570327    0.410611    0.447605    0.384234   \nstd      0.276070    0.271096    0.215362    0.250603    0.270478    0.194463   \nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n25%      0.156752    0.214360    0.440336    0.202703    0.220777    0.242059   \n50%      0.353636    0.387397    0.593013    0.348536    0.428724    0.369840   \n75%      0.636449    0.687500    0.724365    0.611205    0.664469    0.522813   \nmax      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n\n                6  \ncount  208.000000  \nmean     0.439266  \nstd      0.242573  \nmin      0.000000  \n25%      0.258986  \n50%      0.348104  \n75%      0.668882  \nmax      1.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.401830</td>\n      <td>0.443778</td>\n      <td>0.570327</td>\n      <td>0.410611</td>\n      <td>0.447605</td>\n      <td>0.384234</td>\n      <td>0.439266</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.276070</td>\n      <td>0.271096</td>\n      <td>0.215362</td>\n      <td>0.250603</td>\n      <td>0.270478</td>\n      <td>0.194463</td>\n      <td>0.242573</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.156752</td>\n      <td>0.214360</td>\n      <td>0.440336</td>\n      <td>0.202703</td>\n      <td>0.220777</td>\n      <td>0.242059</td>\n      <td>0.258986</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.353636</td>\n      <td>0.387397</td>\n      <td>0.593013</td>\n      <td>0.348536</td>\n      <td>0.428724</td>\n      <td>0.369840</td>\n      <td>0.348104</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.636449</td>\n      <td>0.687500</td>\n      <td>0.724365</td>\n      <td>0.611205</td>\n      <td>0.664469</td>\n      <td>0.522813</td>\n      <td>0.668882</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 582
    }
   ],
   "source": [
    "dict_to_replace = {1:np.int(0), 2:np.int(1), 3:np.int(2)}\n",
    "y = df[\"Type\"]\n",
    "y = y.replace(dict_to_replace)\n",
    "print(y[:5])\n",
    "\n",
    "X = df.drop(\"Type\", axis=1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler() \n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(data=X)\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.34938621 0.34710744 0.87931034 ... 0.25145302 0.1506647  0.        ]\n [0.3068933  0.3161157  0.79310345 ... 0.19424255 0.14081733 0.        ]\n [0.52407932 0.53305785 0.86479129 ... 0.07670104 0.3229936  0.        ]\n ...\n [0.24645892 0.25826446 0.7277677  ... 0.98166664 0.26440177 2.        ]\n [0.11803588 0.16528926 0.39927405 ... 0.36834441 0.25849335 2.        ]\n [0.16147309 0.19214876 0.54718693 ... 0.63346292 0.26784835 2.        ]]\n"
    }
   ],
   "source": [
    "data = pd.concat((X, y),axis=1)\n",
    "data = data.to_numpy()\n",
    "\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n(166, 8)\n(42, 8)\n[[0.07082152974504252, 0.09504132231404938, 0.4673321234119783, 0.08671171171171155, 0.15609408410548808, 0.3357084346435398, 0.23830625307730147, 2], [0.14353163361661947, 0.17768595041322266, 0.5063520871143368, 0.18975225225225278, 0.24590163934426257, 0.4377771132117178, 0.2427375677006398, 2], [0.47025495750708224, 0.5661157024793386, 0.40471869328493604, 0.5748873873873874, 0.4283677833214543, 0.24378161203500243, 0.6696208764155585, 1]]\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train, test = tts(data, train_size=0.8, random_state=2)\n",
    "\n",
    "\n",
    "print(type(train))\n",
    "print(type(test))\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "\n",
    "train = train.tolist()\n",
    "test = test.tolist()\n",
    "\n",
    "for row in train:\n",
    "    row[-1] = int(row[-1])\n",
    "\n",
    "for row in test:\n",
    "    row[-1] = int(row[-1])\n",
    "\n",
    "print(test[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[\n    [\n        {\n            \"bias\": 0.2550690257394217,\n            \"weights\": [\n                0.13436424411240122,\n                0.8474337369372327,\n                0.763774618976614\n            ]\n        },\n        {\n            \"bias\": 0.7887233511355132,\n            \"weights\": [\n                0.49543508709194095,\n                0.4494910647887381,\n                0.651592972722763\n            ]\n        }\n    ],\n    [\n        {\n            \"bias\": 0.8357651039198697,\n            \"weights\": [\n                0.0938595867742349,\n                0.02834747652200631\n            ]\n        },\n        {\n            \"bias\": 0.0021060533511106927,\n            \"weights\": [\n                0.43276706790505337,\n                0.762280082457942\n            ]\n        },\n        {\n            \"bias\": 0.22876222127045265,\n            \"weights\": [\n                0.4453871940548014,\n                0.7215400323407826\n            ]\n        },\n        {\n            \"bias\": 0.030589983033553536,\n            \"weights\": [\n                0.9452706955539223,\n                0.9014274576114836\n            ]\n        },\n        {\n            \"bias\": 0.9391491627785106,\n            \"weights\": [\n                0.0254458609934608,\n                0.5414124727934966\n            ]\n        }\n    ],\n    [\n        {\n            \"bias\": 0.43788759365057206,\n            \"weights\": [\n                0.38120423768821243,\n                0.21659939713061338,\n                0.4221165755827173,\n                0.029040787574867943,\n                0.22169166627303505\n            ]\n        },\n        {\n            \"bias\": 0.28978161459048557,\n            \"weights\": [\n                0.49581224138185065,\n                0.23308445025757263,\n                0.2308665415409843,\n                0.2187810373376886,\n                0.4596034657377336\n            ]\n        },\n        {\n            \"bias\": 0.9925434121760651,\n            \"weights\": [\n                0.021489705265908876,\n                0.8375779756625729,\n                0.5564543226524334,\n                0.6422943629324456,\n                0.1859062658947177\n            ]\n        }\n    ],\n    [\n        {\n            \"bias\": 0.7214844075832684,\n            \"weights\": [\n                0.8599465287952899,\n                0.12088995980580641,\n                0.3326951853601291\n            ]\n        },\n        {\n            \"bias\": 0.830035693274327,\n            \"weights\": [\n                0.7111917696952796,\n                0.9364405867994596,\n                0.4221069999614152\n            ]\n        },\n        {\n            \"bias\": 0.8824790008318577,\n            \"weights\": [\n                0.670305566414071,\n                0.3033685109329176,\n                0.5875806061435594\n            ]\n        }\n    ]\n]\nNone\n"
    }
   ],
   "source": [
    "from random import random, seed\n",
    "import json\n",
    "\n",
    "def print_json_dump(dump):\n",
    "    print(json.dumps(dump, sort_keys=True, indent=4))\n",
    "\n",
    "# generate weights and bias\n",
    "def create_network(n_inputs:int, n_hidden_layers:int, n_neurons_for_layer:list, n_outputs:int):\n",
    "    \"\"\"Creates a neural network with layers, neurons with weights and bias, output neurons with weights and bias\n",
    "\n",
    "    Args:\n",
    "        n_inputs (int): The amount of input features\n",
    "        n_hidden_layers (int): The amount of hidden layers desired\n",
    "        n_neurons_for_layer (list): A list containing the number of neurons per hidden layer\n",
    "        n_outputs (int): Amount of output neurons wanted\n",
    "\n",
    "    Returns:\n",
    "        (list): Your neural network\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(n_neurons_for_layer) == n_hidden_layers, \\\n",
    "        (\"The length of this list needs to be the same as n_hidden_layers\")\n",
    "\n",
    "    network = []\n",
    "    current_layer = -1\n",
    "\n",
    "    for hidden_layer in range(n_hidden_layers):\n",
    "        current_layer += 1\n",
    "        layer = []\n",
    "        for nodes in range(n_neurons_for_layer[current_layer]):\n",
    "            if current_layer == 0:\n",
    "                weights = [random() for i in range(n_inputs)]\n",
    "            elif current_layer > 0:\n",
    "                weights = [random() for i in range(n_neurons_for_layer[current_layer-1])]\n",
    "\n",
    "            bias = random()\n",
    "            node = {\"weights\":weights, \"bias\":bias}\n",
    "            layer.append(node)\n",
    "\n",
    "        network.append(layer)\n",
    "\n",
    "    \n",
    "    n_output_weights = len(network[-1])\n",
    "    layer = []\n",
    "    for i in range(n_outputs):\n",
    "        weights = [random() for k in range(n_output_weights)]\n",
    "        bias = random()\n",
    "        node = {\"weights\":weights, \"bias\":bias}\n",
    "        layer.append(node)\n",
    "\n",
    "    network.append(layer)\n",
    "\n",
    "    return network\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "\n",
    "seed(1)\n",
    "my_net = create_network(3, 3, [2, 5, 3], 3)\n",
    "# my_net = initialize_network(7, 10, 3)\n",
    "# print(my_net)\n",
    "print(print_json_dump(my_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Neuron activation using sigmoid function\n",
    "def sigmoid(x):\n",
    "    val = 1/(1+np.exp(-x)) #maybe replace with math.exp\n",
    "    return val\n",
    "\n",
    "# Calculate the derivative of a neuron output\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Calculate neuron activation for in input\n",
    "def activate(weights, bias, inputs):\n",
    "    activation = bias\n",
    "    for i in range(len(weights)):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron[\"weights\"], neuron[\"bias\"], inputs)\n",
    "            neuron[\"output\"] = sigmoid(activation)\n",
    "            new_inputs.append(neuron[\"output\"])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "\n",
    "# Update weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron[\"output\"] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron[\"weights\"][j] += l_rate * neuron[\"delta\"] * inputs[j]\n",
    "            neuron[\"bias\"] += l_rate * neuron[\"delta\"]\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[int(row[-1])] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print(f'>epoch={epoch}, lrate={l_rate}, error={sum_error}')\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Calculate accuracy\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Backpropagation Algorithm with stochastic gradient descent\n",
    "def back_propagation(train, l_rate, n_epoch, n_hidden_layers, n_neurons_per_layer):\n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    network = create_network(n_inputs, n_hidden_layers, n_neurons_per_layer, n_outputs)\n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n>epoch=596, lrate=0.1, error=37.19399956605829\n>epoch=597, lrate=0.1, error=37.085480309979474\n>epoch=598, lrate=0.1, error=36.977374972130406\n>epoch=599, lrate=0.1, error=36.86970158708603\n>epoch=600, lrate=0.1, error=36.762477687504784\n>epoch=601, lrate=0.1, error=36.65572029211122\n>epoch=602, lrate=0.1, error=36.549445894839685\n>epoch=603, lrate=0.1, error=36.443670455162525\n>epoch=604, lrate=0.1, error=36.33840938962085\n>epoch=605, lrate=0.1, error=36.23367756456924\n>epoch=606, lrate=0.1, error=36.12948929014019\n>epoch=607, lrate=0.1, error=36.02585831542656\n>epoch=608, lrate=0.1, error=35.92279782487621\n>epoch=609, lrate=0.1, error=35.82032043588457\n>epoch=610, lrate=0.1, error=35.71843819756587\n>epoch=611, lrate=0.1, error=35.617162590679655\n>epoch=612, lrate=0.1, error=35.516504528680805\n>epoch=613, lrate=0.1, error=35.41647435985854\n>epoch=614, lrate=0.1, error=35.31708187052506\n>epoch=615, lrate=0.1, error=35.218336289209276\n>epoch=616, lrate=0.1, error=35.12024629180854\n>epoch=617, lrate=0.1, error=35.022820007647404\n>epoch=618, lrate=0.1, error=34.926065026390425\n>epoch=619, lrate=0.1, error=34.82998840575299\n>epoch=620, lrate=0.1, error=34.734596679953995\n>epoch=621, lrate=0.1, error=34.63989586885113\n>epoch=622, lrate=0.1, error=34.5458914877008\n>epoch=623, lrate=0.1, error=34.45258855748346\n>epoch=624, lrate=0.1, error=34.35999161573576\n>epoch=625, lrate=0.1, error=34.2681047278316\n>epoch=626, lrate=0.1, error=34.176931498655\n>epoch=627, lrate=0.1, error=34.08647508460987\n>epoch=628, lrate=0.1, error=33.99673820591239\n>epoch=629, lrate=0.1, error=33.907723159114525\n>epoch=630, lrate=0.1, error=33.819431829809\n>epoch=631, lrate=0.1, error=33.731865705468664\n>epoch=632, lrate=0.1, error=33.64502588837532\n>epoch=633, lrate=0.1, error=33.558913108596066\n>epoch=634, lrate=0.1, error=33.47352773696772\n>epoch=635, lrate=0.1, error=33.38886979805285\n>epoch=636, lrate=0.1, error=33.304938983033146\n>epoch=637, lrate=0.1, error=33.22173466250916\n>epoch=638, lrate=0.1, error=33.139255899178195\n>epoch=639, lrate=0.1, error=33.05750146036415\n>epoch=640, lrate=0.1, error=32.97646983037609\n>epoch=641, lrate=0.1, error=32.89615922267512\n>epoch=642, lrate=0.1, error=32.81656759183088\n>epoch=643, lrate=0.1, error=32.73769264525159\n>epoch=644, lrate=0.1, error=32.659531854674086\n>epoch=645, lrate=0.1, error=32.582082467401406\n>epoch=646, lrate=0.1, error=32.505341517278076\n>epoch=647, lrate=0.1, error=32.42930583539564\n>epoch=648, lrate=0.1, error=32.353972060520604\n>epoch=649, lrate=0.1, error=32.279336649240996\n>epoch=650, lrate=0.1, error=32.205395885827166\n>epoch=651, lrate=0.1, error=32.13214589180517\n>epoch=652, lrate=0.1, error=32.059582635240965\n>epoch=653, lrate=0.1, error=31.9877019397358\n>epoch=654, lrate=0.1, error=31.91649949313376\n>epoch=655, lrate=0.1, error=31.845970855943026\n>epoch=656, lrate=0.1, error=31.776111469473072\n>epoch=657, lrate=0.1, error=31.706916663691967\n>epoch=658, lrate=0.1, error=31.638381664806495\n>epoch=659, lrate=0.1, error=31.57050160257\n>epoch=660, lrate=0.1, error=31.503271517322656\n>epoch=661, lrate=0.1, error=31.436686366769\n>epoch=662, lrate=0.1, error=31.370741032498554\n>epoch=663, lrate=0.1, error=31.305430326254694\n>epoch=664, lrate=0.1, error=31.2407489959584\n>epoch=665, lrate=0.1, error=31.17669173149204\n>epoch=666, lrate=0.1, error=31.113253170250324\n>epoch=667, lrate=0.1, error=31.050427902463774\n>epoch=668, lrate=0.1, error=30.98821047630181\n>epoch=669, lrate=0.1, error=30.926595402761183\n>epoch=670, lrate=0.1, error=30.865577160346447\n>epoch=671, lrate=0.1, error=30.805150199548777\n>epoch=672, lrate=0.1, error=30.74530894712893\n>epoch=673, lrate=0.1, error=30.6860478102115\n>epoch=674, lrate=0.1, error=30.627361180195116\n>epoch=675, lrate=0.1, error=30.569243436486286\n>epoch=676, lrate=0.1, error=30.51168895006155\n>epoch=677, lrate=0.1, error=30.454692086864153\n>epoch=678, lrate=0.1, error=30.398247211041117\n>epoch=679, lrate=0.1, error=30.342348688026032\n>epoch=680, lrate=0.1, error=30.28699088747301\n>epoch=681, lrate=0.1, error=30.232168186047353\n>epoch=682, lrate=0.1, error=30.177874970077653\n>epoch=683, lrate=0.1, error=30.124105638074706\n>epoch=684, lrate=0.1, error=30.070854603122125\n>epoch=685, lrate=0.1, error=30.01811629514317\n>epoch=686, lrate=0.1, error=29.9658851630485\n>epoch=687, lrate=0.1, error=29.914155676769568\n>epoch=688, lrate=0.1, error=29.862922329181472\n>epoch=689, lrate=0.1, error=29.812179637919904\n>epoch=690, lrate=0.1, error=29.76192214709627\n>epoch=691, lrate=0.1, error=29.712144428914428\n>epoch=692, lrate=0.1, error=29.662841085193577\n>epoch=693, lrate=0.1, error=29.614006748800346\n>epoch=694, lrate=0.1, error=29.56563608499384\n>epoch=695, lrate=0.1, error=29.517723792687544\n>epoch=696, lrate=0.1, error=29.47026460563057\n>epoch=697, lrate=0.1, error=29.423253293512033\n>epoch=698, lrate=0.1, error=29.376684662991664\n>epoch=699, lrate=0.1, error=29.33055355865934\n>epoch=700, lrate=0.1, error=29.28485486392676\n>epoch=701, lrate=0.1, error=29.239583501853694\n>epoch=702, lrate=0.1, error=29.1947344359118\n>epoch=703, lrate=0.1, error=29.150302670688568\n>epoch=704, lrate=0.1, error=29.10628325253361\n>epoch=705, lrate=0.1, error=29.06267127015017\n>epoch=706, lrate=0.1, error=29.019461855133713\n>epoch=707, lrate=0.1, error=28.976650182460318\n>epoch=708, lrate=0.1, error=28.93423147092641\n>epoch=709, lrate=0.1, error=28.89220098354254\n>epoch=710, lrate=0.1, error=28.85055402788304\n>epoch=711, lrate=0.1, error=28.809285956393087\n>epoch=712, lrate=0.1, error=28.76839216665572\n>epoch=713, lrate=0.1, error=28.72786810162002\n>epoch=714, lrate=0.1, error=28.68770924979251\n>epoch=715, lrate=0.1, error=28.64791114539338\n>epoch=716, lrate=0.1, error=28.608469368478968\n>epoch=717, lrate=0.1, error=28.56937954503242\n>epoch=718, lrate=0.1, error=28.530637347023788\n>epoch=719, lrate=0.1, error=28.492238492440492\n>epoch=720, lrate=0.1, error=28.454178745291056\n>epoch=721, lrate=0.1, error=28.41645391558131\n>epoch=722, lrate=0.1, error=28.37905985926639\n>epoch=723, lrate=0.1, error=28.341992478178227\n>epoch=724, lrate=0.1, error=28.305247719930723\n>epoch=725, lrate=0.1, error=28.268821577803113\n>epoch=726, lrate=0.1, error=28.232710090603145\n>epoch=727, lrate=0.1, error=28.19690934251057\n>epoch=728, lrate=0.1, error=28.16141546290251\n>epoch=729, lrate=0.1, error=28.126224626161285\n>epoch=730, lrate=0.1, error=28.091333051465917\n>epoch=731, lrate=0.1, error=28.056737002567814\n>epoch=732, lrate=0.1, error=28.022432787552308\n>epoch=733, lrate=0.1, error=27.988416758585917\n>epoch=734, lrate=0.1, error=27.954685311650742\n>epoch=735, lrate=0.1, error=27.92123488626664\n>epoch=736, lrate=0.1, error=27.88806196520205\n>epoch=737, lrate=0.1, error=27.85516307417381\n>epoch=738, lrate=0.1, error=27.822534781537154\n>epoch=739, lrate=0.1, error=27.790173697966015\n>epoch=740, lrate=0.1, error=27.75807647612501\n>epoch=741, lrate=0.1, error=27.72623981033269\n>epoch=742, lrate=0.1, error=27.69466043621794\n>epoch=743, lrate=0.1, error=27.663335130368665\n>epoch=744, lrate=0.1, error=27.632260709974627\n>epoch=745, lrate=0.1, error=27.601434032464166\n>epoch=746, lrate=0.1, error=27.570851995135275\n>epoch=747, lrate=0.1, error=27.540511534782134\n>epoch=748, lrate=0.1, error=27.510409627317053\n>epoch=749, lrate=0.1, error=27.480543287388226\n>epoch=750, lrate=0.1, error=27.450909567994202\n>epoch=751, lrate=0.1, error=27.421505560094815\n>epoch=752, lrate=0.1, error=27.392328392219856\n>epoch=753, lrate=0.1, error=27.36337523007471\n>epoch=754, lrate=0.1, error=27.3346432761446\n>epoch=755, lrate=0.1, error=27.306129769296493\n>epoch=756, lrate=0.1, error=27.27783198438025\n>epoch=757, lrate=0.1, error=27.24974723182835\n>epoch=758, lrate=0.1, error=27.221872857254976\n>epoch=759, lrate=0.1, error=27.194206241054715\n>epoch=760, lrate=0.1, error=27.166744798000998\n>epoch=761, lrate=0.1, error=27.139485976844725\n>epoch=762, lrate=0.1, error=27.112427259913257\n>epoch=763, lrate=0.1, error=27.085566162709913\n>epoch=764, lrate=0.1, error=27.058900233514365\n>epoch=765, lrate=0.1, error=27.032427052984037\n>epoch=766, lrate=0.1, error=27.006144233756842\n>epoch=767, lrate=0.1, error=26.980049420055366\n>epoch=768, lrate=0.1, error=26.954140287292574\n>epoch=769, lrate=0.1, error=26.928414541679583\n>epoch=770, lrate=0.1, error=26.902869919835194\n>epoch=771, lrate=0.1, error=26.87750418839789\n>epoch=772, lrate=0.1, error=26.852315143639743\n>epoch=773, lrate=0.1, error=26.827300611083224\n>epoch=774, lrate=0.1, error=26.80245844512039\n>epoch=775, lrate=0.1, error=26.777786528634817\n>epoch=776, lrate=0.1, error=26.753282772626385\n>epoch=777, lrate=0.1, error=26.728945115839284\n>epoch=778, lrate=0.1, error=26.704771524392644\n>epoch=779, lrate=0.1, error=26.68075999141481\n>epoch=780, lrate=0.1, error=26.65690853668064\n>epoch=781, lrate=0.1, error=26.63321520625218\n>epoch=782, lrate=0.1, error=26.609678072122957\n>epoch=783, lrate=0.1, error=26.586295231865652\n>epoch=784, lrate=0.1, error=26.563064808283364\n>epoch=785, lrate=0.1, error=26.539984949064714\n>epoch=786, lrate=0.1, error=26.51705382644226\n>epoch=787, lrate=0.1, error=26.49426963685538\n>epoch=788, lrate=0.1, error=26.471630600616116\n>epoch=789, lrate=0.1, error=26.449134961579762\n>epoch=790, lrate=0.1, error=26.426780986818578\n>epoch=791, lrate=0.1, error=26.40456696629996\n>epoch=792, lrate=0.1, error=26.382491212568443\n>epoch=793, lrate=0.1, error=26.360552060431548\n>epoch=794, lrate=0.1, error=26.338747866649875\n>epoch=795, lrate=0.1, error=26.31707700963097\n>epoch=796, lrate=0.1, error=26.29553788912755\n>epoch=797, lrate=0.1, error=26.274128925939426\n>epoch=798, lrate=0.1, error=26.25284856161987\n>epoch=799, lrate=0.1, error=26.231695258185656\n>epoch=800, lrate=0.1, error=26.210667497831388\n>epoch=801, lrate=0.1, error=26.18976378264787\n>epoch=802, lrate=0.1, error=26.16898263434448\n>epoch=803, lrate=0.1, error=26.14832259397559\n>epoch=804, lrate=0.1, error=26.1277822216709\n>epoch=805, lrate=0.1, error=26.10736009637005\n>epoch=806, lrate=0.1, error=26.087054815560993\n>epoch=807, lrate=0.1, error=26.066864995022208\n>epoch=808, lrate=0.1, error=26.046789268569324\n>epoch=809, lrate=0.1, error=26.026826287805164\n>epoch=810, lrate=0.1, error=26.006974721873924\n>epoch=811, lrate=0.1, error=25.987233257219227\n>epoch=812, lrate=0.1, error=25.96760059734588\n>epoch=813, lrate=0.1, error=25.948075462585464\n>epoch=814, lrate=0.1, error=25.928656589865778\n>epoch=815, lrate=0.1, error=25.909342732483815\n>epoch=816, lrate=0.1, error=25.89013265988257\n>epoch=817, lrate=0.1, error=25.871025157431433\n>epoch=818, lrate=0.1, error=25.852019026210154\n>epoch=819, lrate=0.1, error=25.833113082796444\n>epoch=820, lrate=0.1, error=25.8143061590569\n>epoch=821, lrate=0.1, error=25.79559710194164\n>epoch=822, lrate=0.1, error=25.776984773282226\n>epoch=823, lrate=0.1, error=25.7584680495928\n>epoch=824, lrate=0.1, error=25.740045821874965\n>epoch=825, lrate=0.1, error=25.721716995425442\n>epoch=826, lrate=0.1, error=25.703480489647287\n>epoch=827, lrate=0.1, error=25.68533523786428\n>epoch=828, lrate=0.1, error=25.667280187138086\n>epoch=829, lrate=0.1, error=25.649314298089063\n>epoch=830, lrate=0.1, error=25.631436544719488\n>epoch=831, lrate=0.1, error=25.61364591424017\n>epoch=832, lrate=0.1, error=25.59594140689984\n>epoch=833, lrate=0.1, error=25.578322035817415\n>epoch=834, lrate=0.1, error=25.560786826817044\n>epoch=835, lrate=0.1, error=25.543334818266\n>epoch=836, lrate=0.1, error=25.525965060915112\n>epoch=837, lrate=0.1, error=25.50867661774213\n>epoch=838, lrate=0.1, error=25.49146856379744\n>epoch=839, lrate=0.1, error=25.47433998605237\n>epoch=840, lrate=0.1, error=25.45728998325023\n>epoch=841, lrate=0.1, error=25.440317665759522\n>epoch=842, lrate=0.1, error=25.42342215542961\n>epoch=843, lrate=0.1, error=25.406602585448898\n>epoch=844, lrate=0.1, error=25.389858100205164\n>epoch=845, lrate=0.1, error=25.373187855148085\n>epoch=846, lrate=0.1, error=25.356591016654246\n>epoch=847, lrate=0.1, error=25.340066761893983\n>epoch=848, lrate=0.1, error=25.323614278700337\n>epoch=849, lrate=0.1, error=25.307232765440432\n>epoch=850, lrate=0.1, error=25.290921430888467\n>epoch=851, lrate=0.1, error=25.274679494100905\n>epoch=852, lrate=0.1, error=25.258506184293235\n>epoch=853, lrate=0.1, error=25.242400740719233\n>epoch=854, lrate=0.1, error=25.226362412551396\n>epoch=855, lrate=0.1, error=25.210390458763534\n>epoch=856, lrate=0.1, error=25.194484148014944\n>epoch=857, lrate=0.1, error=25.178642758536448\n>epoch=858, lrate=0.1, error=25.16286557801784\n>epoch=859, lrate=0.1, error=25.147151903497235\n>epoch=860, lrate=0.1, error=25.1315010412517\n>epoch=861, lrate=0.1, error=25.11591230668962\n>epoch=862, lrate=0.1, error=25.100385024244556\n>epoch=863, lrate=0.1, error=25.084918527270403\n>epoch=864, lrate=0.1, error=25.069512157938217\n>epoch=865, lrate=0.1, error=25.054165267134152\n>epoch=866, lrate=0.1, error=25.038877214358962\n>epoch=867, lrate=0.1, error=25.0236473676288\n>epoch=868, lrate=0.1, error=25.008475103377084\n>epoch=869, lrate=0.1, error=24.993359806358022\n>epoch=870, lrate=0.1, error=24.978300869550853\n>epoch=871, lrate=0.1, error=24.96329769406571\n>epoch=872, lrate=0.1, error=24.948349689050435\n>epoch=873, lrate=0.1, error=24.933456271598704\n>epoch=874, lrate=0.1, error=24.918616866658684\n>epoch=875, lrate=0.1, error=24.90383090694372\n>epoch=876, lrate=0.1, error=24.889097832843287\n>epoch=877, lrate=0.1, error=24.874417092335104\n>epoch=878, lrate=0.1, error=24.859788140898658\n>epoch=879, lrate=0.1, error=24.845210441429202\n>epoch=880, lrate=0.1, error=24.830683464152965\n>epoch=881, lrate=0.1, error=24.81620668654324\n>epoch=882, lrate=0.1, error=24.801779593237296\n>epoch=883, lrate=0.1, error=24.787401675954452\n>epoch=884, lrate=0.1, error=24.77307243341452\n>epoch=885, lrate=0.1, error=24.758791371257573\n>epoch=886, lrate=0.1, error=24.744558001964386\n>epoch=887, lrate=0.1, error=24.730371844777444\n>epoch=888, lrate=0.1, error=24.71623242562306\n>epoch=889, lrate=0.1, error=24.70213927703414\n>epoch=890, lrate=0.1, error=24.688091938073736\n>epoch=891, lrate=0.1, error=24.67408995425924\n>epoch=892, lrate=0.1, error=24.660132877487328\n>epoch=893, lrate=0.1, error=24.646220265959798\n>epoch=894, lrate=0.1, error=24.632351684109867\n>epoch=895, lrate=0.1, error=24.61852670252923\n>epoch=896, lrate=0.1, error=24.604744897895895\n>epoch=897, lrate=0.1, error=24.591005852902402\n>epoch=898, lrate=0.1, error=24.577309156185155\n>epoch=899, lrate=0.1, error=24.563654402253874\n>epoch=900, lrate=0.1, error=24.55004119142197\n>epoch=901, lrate=0.1, error=24.536469129737533\n>epoch=902, lrate=0.1, error=24.522937828914806\n>epoch=903, lrate=0.1, error=24.5094469062663\n>epoch=904, lrate=0.1, error=24.495995984635563\n>epoch=905, lrate=0.1, error=24.482584692330455\n>epoch=906, lrate=0.1, error=24.46921266305699\n>epoch=907, lrate=0.1, error=24.455879535853796\n>epoch=908, lrate=0.1, error=24.442584955027158\n>epoch=909, lrate=0.1, error=24.429328570086412\n>epoch=910, lrate=0.1, error=24.416110035680187\n>epoch=911, lrate=0.1, error=24.40292901153297\n>epoch=912, lrate=0.1, error=24.389785162382186\n>epoch=913, lrate=0.1, error=24.37667815791604\n>epoch=914, lrate=0.1, error=24.363607672711574\n>epoch=915, lrate=0.1, error=24.35057338617354\n>epoch=916, lrate=0.1, error=24.33757498247346\n>epoch=917, lrate=0.1, error=24.324612150489564\n>epoch=918, lrate=0.1, error=24.31168458374693\n>epoch=919, lrate=0.1, error=24.298791980358295\n>epoch=920, lrate=0.1, error=24.285934042965227\n>epoch=921, lrate=0.1, error=24.27311047867993\n>epoch=922, lrate=0.1, error=24.26032099902756\n>epoch=923, lrate=0.1, error=24.247565319888857\n>epoch=924, lrate=0.1, error=24.234843161443354\n>epoch=925, lrate=0.1, error=24.22215424811316\n>epoch=926, lrate=0.1, error=24.209498308507143\n>epoch=927, lrate=0.1, error=24.196875075365483\n>epoch=928, lrate=0.1, error=24.184284285504884\n>epoch=929, lrate=0.1, error=24.171725679764187\n>epoch=930, lrate=0.1, error=24.159199002950626\n>epoch=931, lrate=0.1, error=24.14670400378597\n>epoch=932, lrate=0.1, error=24.134240434854053\n>epoch=933, lrate=0.1, error=24.121808052547856\n>epoch=934, lrate=0.1, error=24.109406617017793\n>epoch=935, lrate=0.1, error=24.097035892119962\n>epoch=936, lrate=0.1, error=24.084695645365063\n>epoch=937, lrate=0.1, error=24.07238564786789\n>epoch=938, lrate=0.1, error=24.06010567429695\n>epoch=939, lrate=0.1, error=24.04785550282498\n>epoch=940, lrate=0.1, error=24.035634915079463\n>epoch=941, lrate=0.1, error=24.023443696094006\n>epoch=942, lrate=0.1, error=24.011281634260015\n>epoch=943, lrate=0.1, error=23.99914852127862\n>epoch=944, lrate=0.1, error=23.987044152113466\n>epoch=945, lrate=0.1, error=23.974968324943617\n>epoch=946, lrate=0.1, error=23.962920841117157\n>epoch=947, lrate=0.1, error=23.95090150510498\n>epoch=948, lrate=0.1, error=23.938910124455347\n>epoch=949, lrate=0.1, error=23.926946509748547\n>epoch=950, lrate=0.1, error=23.915010474552343\n>epoch=951, lrate=0.1, error=23.903101835377715\n>epoch=952, lrate=0.1, error=23.891220411634876\n>epoch=953, lrate=0.1, error=23.87936602559014\n>epoch=954, lrate=0.1, error=23.867538502322837\n>epoch=955, lrate=0.1, error=23.855737669682952\n>epoch=956, lrate=0.1, error=23.843963358248978\n>epoch=957, lrate=0.1, error=23.832215401286483\n>epoch=958, lrate=0.1, error=23.820493634706832\n>epoch=959, lrate=0.1, error=23.808797897026565\n>epoch=960, lrate=0.1, error=23.797128029327123\n>epoch=961, lrate=0.1, error=23.78548387521495\n>epoch=962, lrate=0.1, error=23.773865280782303\n>epoch=963, lrate=0.1, error=23.762272094567965\n>epoch=964, lrate=0.1, error=23.750704167519114\n>epoch=965, lrate=0.1, error=23.739161352952976\n>epoch=966, lrate=0.1, error=23.727643506519186\n>epoch=967, lrate=0.1, error=23.716150486162697\n>epoch=968, lrate=0.1, error=23.704682152086814\n>epoch=969, lrate=0.1, error=23.693238366717047\n>epoch=970, lrate=0.1, error=23.68181899466495\n>epoch=971, lrate=0.1, error=23.67042390269261\n>epoch=972, lrate=0.1, error=23.659052959677638\n>epoch=973, lrate=0.1, error=23.647706036578562\n>epoch=974, lrate=0.1, error=23.63638300640031\n>epoch=975, lrate=0.1, error=23.62508374416057\n>epoch=976, lrate=0.1, error=23.613808126856213\n>epoch=977, lrate=0.1, error=23.60255603343045\n>epoch=978, lrate=0.1, error=23.591327344739952\n>epoch=979, lrate=0.1, error=23.580121943522926\n>epoch=980, lrate=0.1, error=23.56893971436701\n>epoch=981, lrate=0.1, error=23.55778054367815\n>epoch=982, lrate=0.1, error=23.5466443196494\n>epoch=983, lrate=0.1, error=23.535530932230458\n>epoch=984, lrate=0.1, error=23.524440273097298\n>epoch=985, lrate=0.1, error=23.513372235622562\n>epoch=986, lrate=0.1, error=23.50232671484591\n>epoch=987, lrate=0.1, error=23.491303607445175\n>epoch=988, lrate=0.1, error=23.480302811707585\n>epoch=989, lrate=0.1, error=23.46932422750162\n>epoch=990, lrate=0.1, error=23.458367756248993\n>epoch=991, lrate=0.1, error=23.447433300897234\n>epoch=992, lrate=0.1, error=23.436520765892524\n>epoch=993, lrate=0.1, error=23.425630057152954\n>epoch=994, lrate=0.1, error=23.414761082042165\n>epoch=995, lrate=0.1, error=23.40391374934328\n>epoch=996, lrate=0.1, error=23.39308796923324\n>epoch=997, lrate=0.1, error=23.382283653257574\n>epoch=998, lrate=0.1, error=23.37150071430544\n>epoch=999, lrate=0.1, error=23.360739066585015\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=2\nExpected=1, Got=1\nExpected=0, Got=2\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=1\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=0, Got=1\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=2, Got=2\nExpected=0, Got=0\nExpected=1, Got=1\nExpected=2, Got=2\nExpected=2, Got=2\nExpected=1, Got=1\nExpected=2, Got=0\nAccuracy: 88.09523809523809\n"
    }
   ],
   "source": [
    "l_rate = 0.1\n",
    "n_epoch = 10000\n",
    "n_hidden_layers = 2\n",
    "n_neurons_per_layer = [10, 10]\n",
    "\n",
    "# print(train[:5])\n",
    "\n",
    "# Train model\n",
    "Model = back_propagation(train, l_rate, n_epoch, n_hidden_layers, n_neurons_per_layer)\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "PredClass = list()\n",
    "ActualClass = list()\n",
    "for row in test:\n",
    "    prediction = predict(Model, row)\n",
    "    PredClass.append(prediction)\n",
    "    ActualClass.append(row[-1])\n",
    "    print('Expected=%d, Got=%d' % (row[-1], prediction))\n",
    "\n",
    "accuracy = accuracy_metric(ActualClass, PredClass)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}