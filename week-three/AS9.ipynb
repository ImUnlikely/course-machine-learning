{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitc4b27e4dae3c4699860ff02916ead089",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of:\n",
    "https://medium.com/towards-artificial-intelligence/main-types-of-neural-networks-and-its-applications-tutorial-734480d7ec8e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron (P):\n",
    "### About:\n",
    "- Single layer (2 input cells and 1 output cell)\n",
    "\n",
    "### Application:\n",
    "- Classification\n",
    "- Encode Database (Multilayer Perceptron)\n",
    "- Monitor Access Data (Multilayer Perceptron)\n",
    "\n",
    "\n",
    "## 2. Feed Forward (FF):\n",
    "### About:\n",
    "- Every perceptron in one layer is connected with each node in the next layer\n",
    "- No back-loops in Feed Forward network\n",
    "- Uses backpropagation algorithm to update weights\n",
    "\n",
    "### Application:\n",
    "- Data compression\n",
    "- Pattern Recognition\n",
    "- Computer Vision\n",
    "- Sonar Target Recognition\n",
    "- Speech Recognition\n",
    "- Handwritten Characters Recognition\n",
    "\n",
    "\n",
    "## 3. Radial Basis Network (RBN):\n",
    "### About:\n",
    "- Distiguiashable from other Neural Networks because of their faster learning rate and universal approximation\n",
    "- Uses Radial Basis Function as an activation function\n",
    "\n",
    "### Application:\n",
    "- Function Approximation\n",
    "- Timeseries Prediction\n",
    "- Classification\n",
    "- System control\n",
    "\n",
    "\n",
    "## 4. Deep Feed-forrward (DFF):\n",
    "### About:\n",
    "- Feed forward network that uses more than one hidden layer\n",
    "\n",
    "### Application:\n",
    "- Data Compression\n",
    "- Pattern Recognition\n",
    "- Computer Vision\n",
    "- ECG Noise Filtering\n",
    "- Financial Prediction\n",
    "\n",
    "\n",
    "## 5. Recurrent Nearal Network (RNN):\n",
    "### About:\n",
    "- A variation to feed forward networks\n",
    "- Slow computational speed\n",
    "- Cannot consider any future input for the current state\n",
    "- Cannot remember info from a long time ago\n",
    "\n",
    "### Application:\n",
    "- Machine Translation\n",
    "- Robot Control\n",
    "- Time Series Prediction\n",
    "- Speech Recognition\n",
    "- Speech Synthesis\n",
    "- Time Series Anomaly Detection\n",
    "- Rhythm Learning\n",
    "- Music Composition\n",
    "\n",
    "\n",
    "## 6. Long / Short Term Memory (LSTM):\n",
    "### About:\n",
    "- Can process data with memory gaps\n",
    "- If RNN fails when we have a large number of relevant data, and we want to find out relevant data from it, then LSTMs is the way to go.\n",
    "\n",
    "### Application:\n",
    "- Speech Recognition\n",
    "- Writing Recognition\n",
    "\n",
    "\n",
    "## 7. Gated Recurrent Unit (GRU):\n",
    "### About:\n",
    "- A variation of LSTMs\n",
    "- Only three gates, and they do not maintain an Intenal Cell State\n",
    "\n",
    "### Application:\n",
    "- Polyphonic Music Modeling\n",
    "- Speech Signal Modeling\n",
    "- Natural Language Processing\n",
    "\n",
    "\n",
    "## 8. Auto Encoder (AE):\n",
    "### About:\n",
    "- Finds common patterns and generalizes the data\n",
    "\n",
    "### Application:\n",
    "- Classification\n",
    "- Clustering\n",
    "- Feature Compression\n",
    "\n",
    "\n",
    "## 9. Variational Autoencoder (VAE):\n",
    "### About:\n",
    "- Uses probabilistic approach for describing observations\n",
    "\n",
    "### Application:\n",
    "- Interpolate Between Sentences\n",
    "- Automate Image Generation\n",
    "\n",
    "\n",
    "## 10. Denoising Autoencoder (DAE):\n",
    "### About:\n",
    "- forces the hidden layer to learn more robust features so that the output is a more refined version of the noisy input\n",
    "\n",
    "### Application:\n",
    "- Feature Extraction\n",
    "- Dimensionality Reduction\n",
    "\n",
    "\n",
    "## 11. Sparse Autoencoder (SAE):\n",
    "### About:\n",
    "- Construct loss function by penalizing activations of hidden layers so that only a few nodes are activated when a single sample is fed into the network\n",
    "\n",
    "### Application:\n",
    "- Feature Extraction\n",
    "- Handwriting digits Recognition \n",
    "\n",
    "\n",
    "## 12. Markov Chain (MC):\n",
    "### About:\n",
    "- A mathematical system that experiences the transition from oe state to another based on some probabilistic rules.\n",
    "\n",
    "Some of the possible states can be:\n",
    "\n",
    "- Letters\n",
    "- Numbers\n",
    "- Weather Conditions\n",
    "- Baseball Scores\n",
    "- Stock Performace\n",
    "\n",
    "### Application:\n",
    "- Speech Recognition\n",
    "- Information And Communication System\n",
    "- Queuing Theory\n",
    "- Statistics\n",
    "\n",
    "\n",
    "## 13. Hopfield Network (HN):\n",
    "### About:\n",
    "- A neuron is either ON or OFF\n",
    "- The state of the neurons can change by receiving inputs from other neurons\n",
    "- Generally used to store patterns and memories\n",
    "\n",
    "### Application:\n",
    "- Optimization Problems\n",
    "- Image Detection And Recognition\n",
    "- Medical Image Recognition\n",
    "- Enhacing X-Ray Images\n",
    "\n",
    "\n",
    "## 14. Boltzmann Machine (BM):\n",
    "### About:\n",
    "- Check link\n",
    "\n",
    "### Application:\n",
    "- Dimensionality Reduction\n",
    "- Classification\n",
    "- Regression\n",
    "- Collaborative Filtering\n",
    "- Feature Learning\n",
    "\n",
    "\n",
    "## 15. Restricted Boltzmann Machine (RBM):\n",
    "### About:\n",
    "- No internal connections inside each layer\n",
    "\n",
    "### Application:\n",
    "- Filtering\n",
    "- Feature Learning\n",
    "- Classification\n",
    "- Risk Detection\n",
    "- Business and Economic analysis\n",
    "\n",
    "\n",
    "## 16. Deep Belief Network (DBN):\n",
    "### About:\n",
    "- Many hidden layers\n",
    "- Layers act as feature detectors\n",
    "\n",
    "### Application:\n",
    "- Retrieval of Documents / Images\n",
    "- Non-linear Dimensionality Reduction\n",
    "\n",
    "\n",
    "## 17. Deep Convolutional Network (DCN):\n",
    "### About:\n",
    "- Primarily used for classification of images, clustering of images and object recognition\n",
    "\n",
    "### Application:\n",
    "- Identify Faces, Street Signs, Tumors\n",
    "- Image Recognition\n",
    "- Video Analysis\n",
    "- NLP.\n",
    "- Anomaly Detection\n",
    "- Drug Discovery\n",
    "- Checkers Game\n",
    "- Time Series Forecasting\n",
    "\n",
    "\n",
    "## 18. Deconvolutional Neural Networks (DN):\n",
    "### About:\n",
    "- Helps find lost features or signals in networks\n",
    "\n",
    "### Application:\n",
    "- Image super-resolution\n",
    "- Surface depth estimation from an image\n",
    "- Optical flow estimation\n",
    "\n",
    "\n",
    "## 19. Deep Convolutional Inverse Graphics Network (DC-IGN):\n",
    "### About:\n",
    "- Check link\n",
    "\n",
    "### Application:\n",
    "- Manipulation of human faces\n",
    "\n",
    "\n",
    "## 20. Generative Adversarial Network (GAN):\n",
    "### About:\n",
    "- Learns to generate new data with same statistics as the training data.\n",
    "- Objective is to distinguish between real and synthetic results so that it can generate more authentic results\n",
    "\n",
    "### Application:\n",
    "- Generate New Human Poses\n",
    "- Photes to Emojis\n",
    "- Face Aging\n",
    "- Super Resolution\n",
    "- Clothing Translation\n",
    "- Video Prediction\n",
    "\n",
    "\n",
    "## 21. Liquid State Machine (LSM):\n",
    "### About:\n",
    "- Nodes randomly connect to each other\n",
    "- Extensive collection of neurons\n",
    "\n",
    "### Application:\n",
    "- Speech Recognition\n",
    "- Computer Vision\n",
    "\n",
    "\n",
    "## 22. Extreme Learning Machine (ELM):\n",
    "### About:\n",
    "- Slow learning speed based on gradient algorithms\n",
    "- Tuning all parameters iteratively\n",
    "\n",
    "### Application:\n",
    "- Classification\n",
    "- Regression\n",
    "- Clustering\n",
    "- Sparse Approximation\n",
    "- Feature Learning\n",
    "\n",
    "\n",
    "## 23. Echo State Network (ESN):\n",
    "### About:\n",
    "- Subtype of Recurrent Neural Networks\n",
    "- Each input node receives non-linear signal\n",
    "- Connectivity and weights of hidden nodes are randomly assigned\n",
    "\n",
    "### Application:\n",
    "- Timeseries Prediction\n",
    "- Data Mining\n",
    "\n",
    "\n",
    "## 24. Deep Residual Network (DRN):\n",
    "### About:\n",
    "- Prevents degredation of results\n",
    "- May contain around 300 layers\n",
    "\n",
    "### Application:\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Semantic Segmentation\n",
    "- Speech Recognition\n",
    "- Language Recognition\n",
    "\n",
    "\n",
    "## 25. Kohonen Networks (KN):\n",
    "### About:\n",
    "- Unsupervised\n",
    "- Self-organizing\n",
    "- Used to visualize high dimensionality data\n",
    "- Uses competitive learning rather than error correction learning\n",
    "\n",
    "### Application:\n",
    "- Dimensionality Reduction\n",
    "- Assessment and Prediction of Water Quality\n",
    "- Coastal Water Management\n",
    "\n",
    "\n",
    "## 26. Support Vector Machines (SVM):\n",
    "### About:\n",
    "- Generally used for binary classification\n",
    "- Not generally considered as neural networks\n",
    "\n",
    "### Application:\n",
    "- Face Detection\n",
    "- Text Categorization\n",
    "- Classification\n",
    "- Bioinformatics\n",
    "- Handwriting Recognition\n",
    "\n",
    "\n",
    "## 27. Neural Turing Machine (NTM):\n",
    "### About:\n",
    "- Two primary components (Neural Network Controller and Memory Bank)\n",
    "- Extends the capabilities of standard neural networks by interacting with external memory\n",
    "\n",
    "### Application:\n",
    "- Robotics\n",
    "- Building an artificial human brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of:\n",
    "https://towardsdatascience.com/the-mostly-complete-chart-of-neural-networks-explained-3fb6f2367464\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of:\n",
    "https://www.allerin.com/blog/3-types-of-neural-networks-that-ai-uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literally a copy paste from the website: \n",
    "\n",
    "### 1. Feedforward neural networks\n",
    "Feedforward neural networks are the first type of artificial neural networks to have been created and can be considered as the most commonly used ones today. These neural networks are called feedforward neural networks because the flow of information through the network is unidirectional without going through loops. Feedforward neural networks can further be classified into single-layered networks or multilayered networks, based on the presence of intermediate hidden layers. The number of layers depends on the complexity of the function that needs to be performed. The single-layered feedforward neural network consists of only two layers of neurons and no hidden layers in between them. Multi-layered perceptrons consist of multiple hidden layers between the input and output layers, allowing for multiple stages of information processing.\n",
    "Feedforward neural networks find applications in areas that require supervised learning, such as computer vision. Feedforward neural networks are most commonly used in object recognition and speech recognition systems.\n",
    "\n",
    "\n",
    "### 2. Recurrent neural networks\n",
    "Recurrent neural networks (RNN), as the name suggests, involves the recurrence of operations in the form of loops. These are much more complicated than feedforward networks and can perform more complex tasks than basic image recognition. For instance, recurrent neural networks are usually used in text prediction and language generation. Making sense of and generating natural language involves much more complex processing than image recognition, which recurrent neural networks can perform due to their architecture. While in feedforward neural networks, connections only lead from one neuron to neurons in subsequent layers without any feedback, recurrent neural networks allow for connections to lead back to neurons in the same layer allowing for a broader range of operations.\n",
    "However, conventional RNNs have a few limitations. They are difficult to train and have a very short-term memory, which limits their functionality. To overcome the memory limitation, a newer form of RNN, known as LSTM or Long Short-term Memory networks are used. LSTMs extend the memory RNNs to enable them to perform tasks involving longer-term memory.\n",
    "The main application areas for RNNs include natural languages processing problems such as speech and text recognition, text prediction, and natural language generation.\n",
    "\n",
    "\n",
    "### 3. Convolutional neural networks\n",
    "Convolutional neural networks, ever since its conception has almost exclusively be associated with computer vision applications. That’s because their architecture is specifically suited for performing complex visual analyses. The convolutional neural network architecture is defined by a three-dimensional arrangement of neurons, instead of the standard two-dimensional array. The first layer in such neural networks is called a convolutional layer. Each neuron in the convolutional layer only processes the information from a small part of the visual field. The convolutional layers are followed by rectified layer units or ReLU, which enables the CNN to handle complicated information.\n",
    "CNNs are mainly used in object recognition applications like machine vision and in self-driving vehicles.\n",
    "While these types of artificial neural networks are the most common in today’s AI applications, there are many others that are being innovated to achieve a level of functionality that is more comparable to the human brain. Every new discovery about our brain’s working leads to a new breakthrough in the field of AI, leading to better models of neural networks. Thus, as we continue to understand our brains better, it is only a matter of time before we can reproduce the totality of our brain’s functioning in computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}